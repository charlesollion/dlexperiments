{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further with VAEs\n",
    "\n",
    "Building on the simple VAE exemple, this notebook is organized as follows:\n",
    "1. Convolutional VAE: a stronger architecture, with a few computational tricks to make it work\n",
    "2. [$\\beta$-VAE](https://openreview.net/forum?id=Sy2fzU9gl), increasing the importance of the KL term\n",
    "3. Conditional VAEs, typically conditionning the modeled density by a class\n",
    "4. Exploring [Importance weighted Autoencoders](https://arxiv.org/abs/1509.00519) (IWAE)\n",
    "5. Final thoughts: can VAEs efficiently model simple toy datasets (distributions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(0, 18):\n",
    "    plt.subplot(3, 6, i + 1)\n",
    "    plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "x_train = np.expand_dims(x_train.astype('float32') / 255., -1)\n",
    "x_test = np.expand_dims(x_test.astype('float32') / 255., -1)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad\n",
    "from jax import random\n",
    "rand_key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional VAE\n",
    "\n",
    "A convolutional VAE will make use of the spatial structure of the images, and rely on convolutions instead of Dense layers, both in the encoder and decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gelu = (lambda rng, input_shape: (input_shape, ()), \n",
    "        jit(lambda params, inputs: inputs * jax.nn.sigmoid(1.702*inputs))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax # neural network library\n",
    "from jax.experimental.stax import Dense, Relu, Sigmoid, Conv, BatchNorm, MaxPool, Flatten\n",
    "\n",
    "input_dim = x_train_standard.shape[-1]\n",
    "hidden_dim = 128\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_init, encoder_fn = stax.serial(\n",
    "    Conv(32, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    Conv(32, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    MaxPool((2,2), (2,2)),\n",
    "    Conv(64, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    Conv(64, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    MaxPool((2,2), (2,2)),\n",
    "    Flatten, Dense(latent_dim*2)\n",
    ")\n",
    "\n",
    "#initialize the parameters\n",
    "rand_key, key = random.split(rand_key)\n",
    "out_shape, params_enc = encoder_init(rand_key, (-1, 28, 28, 1))\n",
    "\n",
    "def count_params(params):\n",
    "    count = 0\n",
    "    for param_tuple in params:\n",
    "        for param in param_tuple:\n",
    "            count += np.prod(param.shape)\n",
    "    return count\n",
    "\n",
    "params_num = len(params_enc)\n",
    "print(f\"Number of param objects: {params_num}, total number of params: {count_params(params_enc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time z = jit(encoder_fn)(params_enc, x_train[0:10])\n",
    "%time z = encoder_fn(params_enc, x_train[0:10])\n",
    "print(f\"output shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rand_key, z_mean, z_log_var):\n",
    "    epsilon = random.normal(rand_key, shape=z_mean.shape)\n",
    "    return z_mean + jnp.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "fast_sample = jit(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder_fn(params_enc, x_train[0:3])\n",
    "d = z.shape[-1]//2\n",
    "z_mean, z_log_var = z[:, :d], z[:,d:]\n",
    "rand_key, key = random.split(rand_key)\n",
    "samples = sample(key, z_mean, z_log_var)\n",
    "print(f\"z shape (concatenation of z_mean and z_log_var) : {z.shape}, samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def _upsample(x):\n",
    "    x = x.transpose((0,3,1,2))\n",
    "    upx = jnp.kron(x, jnp.ones((2,2)))\n",
    "    return upx.transpose((0,2,3,1))\n",
    "\n",
    "Upsample = (lambda rng, ish: ((ish[0], ish[1]*2, ish[2]*2, ish[3]), ()), \n",
    "        jit(lambda params, inputs, **kwargs: _upsample(inputs))\n",
    "       )\n",
    "def Reshape(shape):\n",
    "    def init_fun(rng, input_shape):\n",
    "        return (input_shape[0],) + shape, ()\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        return inputs.reshape((inputs.shape[0],) + shape)\n",
    "    \n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reshape((7, 7, 64))[1]((), jnp.ones([2,7*7*64])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(_upsample(x_train[0:3])[0,:,:,0], cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_init, decoder_fn = stax.serial(\n",
    "    Dense(64*7*7), Reshape((7,7,64)),\n",
    "    Upsample, \n",
    "    Conv(32, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    Conv(32, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    Upsample,\n",
    "    Conv(64, (3, 3), padding=\"SAME\"), BatchNorm(), Relu, \n",
    "    Conv(1, (3, 3), padding=\"SAME\"), Sigmoid\n",
    ")\n",
    "\n",
    "#initialize the parameters\n",
    "rand_key, key = random.split(rand_key)\n",
    "out_shape, params_dec = decoder_init(rand_key, (-1, latent_dim))\n",
    "\n",
    "params = params_enc + params_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of params: {count_params(params_dec)}\")\n",
    "decoder_fn(params_dec, samples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder function class prior on image generation\n",
    "rand_key, key = random.split(key)\n",
    "z = random.normal(key, shape=(1,latent_dim))\n",
    "generated = decoder_fn(params_dec, z)\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-6\n",
    "xent = jit(lambda x, xt: - jnp.sum(x * jnp.log(xt + EPSILON) + (1-x)*jnp.log(1-xt+EPSILON), axis=(1,2,3)))\n",
    "kl = jit(lambda z_mean, z_log_var: - 0.5 * jnp.sum(1 + z_log_var - z_mean ** 2 - jnp.exp(z_log_var), axis=(-1)))\n",
    "\n",
    "@jit\n",
    "def vae_loss(rand_key, params, x):\n",
    "    latent = jit(encoder_fn)(params[0:params_num], x)\n",
    "    d = latent.shape[-1]//2\n",
    "    z_mean, z_log_var = latent[:, :d], latent[:,d:]\n",
    "    z_sample = sample(rand_key, z_mean, z_log_var)\n",
    "    x_rec = jit(decoder_fn)(params[params_num:], z_sample)\n",
    "    \n",
    "    xent_loss = xent(x, x_rec)\n",
    "    kl_loss = kl(z_mean, z_log_var)\n",
    "    return jnp.mean(xent_loss + kl_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time vae_loss(rand_key, params, x_train[0:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the VAE\n",
    "\n",
    "The following cells:\n",
    "    - reinitialize parameters\n",
    "    - initialize an Adam optimizer\n",
    "    - run a batch training over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may run this cell to reinit parameters if needed\n",
    "_, params_enc = encoder_init(rand_key, (-1, 28,28,1))\n",
    "_, params_dec = decoder_init(rand_key, (-1, latent_dim))\n",
    "params = params_enc + params_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax, optimizers\n",
    "\n",
    "data_size = x_train.shape[0]\n",
    "batch_size = 32\n",
    "learning_rate = 0.003\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(learning_rate)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(key, batch, opt_state):\n",
    "    params = get_params(opt_state)\n",
    "    value_and_grad_fun = jit(jax.value_and_grad(lambda params, x: vae_loss(key, params, x)))\n",
    "    loss, grads = value_and_grad_fun(params, batch)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(1):\n",
    "    for i in range(10):  #data_size // 32 -1):\n",
    "        batch = x_train[i * 32:(i+1)*32]\n",
    "        rand_key, key = random.split(rand_key)\n",
    "        opt_state, loss = update(key, batch, opt_state)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(opt_state)\n",
    "rand_key, key = random.split(key)\n",
    "z = random.normal(key, shape=(1,latent_dim))\n",
    "generated = decoder_fn(params[params_num:], z)\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D plot of the image classes in the latent space\n",
    "\n",
    "We can also use the encoder to set the visualize the distribution of the test set in the 2D latent space of the VAE model. In the following the colors show the true class labels from the test samples.\n",
    "\n",
    "Note that the VAE is an unsupervised model: it did not use any label information during training. However we can observe that the 2D latent space is largely structured around the categories of images used in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_labels = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", \n",
    "                5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder_fn(params[0:params_num], x_test)\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test,\n",
    "            cmap=plt.cm.tab10)\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks(list(id_to_labels.keys()))\n",
    "cb.set_ticklabels(list(id_to_labels.values()))\n",
    "cb.update_ticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D panel view of samples from the VAE manifold\n",
    "\n",
    "The following linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian to produce values of the latent variables z. This makes it possible to use a square arangement of panels that spans the gaussian prior of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15  # figure with 15x15 panels\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n)).astype(np.float32)\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n)).astype(np.float32)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder_fn(params[params_num:], z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Weighted Autoencoders (IWAE)\n",
    "\n",
    "The idea is simply to sample several latent individuals from the gaussian distribution instead of a single one. Sample $\\{z_i\\}^k \\sim q(z|x)$. The aim is to get a more accurate loss which represents better the distribution instead of a single point MC estimate of $\\mathbb{E} q(z|x)$\n",
    "\n",
    "$$\\mathcal{L}_k = \\sum_i^k  \\tilde{w_i} \\nabla_θ [log p(x, z_i) - log q(z_i|x)]$$\n",
    "\n",
    "$$w_i = \\frac{p(x, z_i)}{q(z_i|x)} ; \\tilde{w_i} = \\frac{w_i}{\\sum_i^k  w_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_samples = 10\n",
    "\n",
    "# Sample now has an additional parameter k_samples\n",
    "def sample(rand_key, z_mean, z_log_var, k_samples):\n",
    "    epsilon = random.normal(rand_key, shape=(k_samples,) + z_mean.shape)\n",
    "    return z_mean + jnp.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "@jit\n",
    "def vae_loss(rand_key, params, x):\n",
    "    latent = jit(encoder_fn)(params[0:3], x)\n",
    "    d = latent.shape[-1]//2\n",
    "    z_mean, z_log_var = latent[:, :d], latent[:,d:]\n",
    "    z_sample = sample(rand_key, z_mean, z_log_var, k_samples)\n",
    "    \n",
    "    # decoding applies to each of the samples\n",
    "    x_rec = jit(decoder_fn)(params[3:], z_sample)\n",
    "    \n",
    "    # these terms apply to each of the samples\n",
    "    xent_loss = xent(x, x_rec)\n",
    "    kl_loss = kl(z_mean, z_log_var)\n",
    "    \n",
    "    # softmax of the log_w_i corresponds to the normalized weights\n",
    "    log_w_i = xent_loss + kl_loss\n",
    "    normalized_w_i = jax.lax.stop_gradient(jax.nn.softmax(xent_loss, axis=0))\n",
    "    \n",
    "    weighted_sum = (normalized_w_i * (xent_loss + kl_loss)).sum(axis=0)\n",
    "    \n",
    "    # average over the batch\n",
    "    loss = jnp.mean(weighted_sum) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time vae_loss(rand_key, params, x_train_standard[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simpler dataset\n",
    "\n",
    "Up to now, we used the VAE on complex structured data (Fashion MNIST), and it can be seen as a dimensionality reduction method, not unlike a [probabilistic PCA](http://edwardlib.org/tutorials/probabilistic-pca).\n",
    "\n",
    "The following explores how a VAE captures the distribution of toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eightgaussian(n_points):\n",
    "    \"\"\"\n",
    "     Returns the eight gaussian dataset.\n",
    "    \"\"\"\n",
    "    n = np.random.randint(0,8, n_points)\n",
    "    noisex = np.random.normal(size=(n_points)) * 0.1\n",
    "    noisey = np.random.normal(size=(n_points)) * 0.1\n",
    "    x_centers,y_centers = [np.cos(n* np.pi/4.0) * 5 + noisex, np.sin(n* np.pi/4.0) * 5 + noisey]\n",
    "    return np.vstack((x_centers,y_centers)).T\n",
    "            \n",
    "X = eightgaussian(1000)\n",
    "X.shape\n",
    "plt.scatter(X[:,0], X[:,1], s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad\n",
    "from jax import random\n",
    "rand_key = random.PRNGKey(1)\n",
    "from jax.experimental import stax # neural network library\n",
    "from jax.experimental.stax import Dense, Relu, Sigmoid, Selu\n",
    "\n",
    "input_dim = X.shape[-1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 2\n",
    "k_samples = 1\n",
    "beta = 1\n",
    "\n",
    "encoder_init, encoder_fn = stax.serial(\n",
    "    Dense(hidden_dim), Selu, Dense(hidden_dim), Selu, Dense(hidden_dim), Selu, Dense(latent_dim * 2))\n",
    "\n",
    "#initialize the parameters\n",
    "rand_key, key = random.split(rand_key)\n",
    "out_shape, params_enc = encoder_init(rand_key, (-1, input_dim))\n",
    "\n",
    "def sample(rand_key, z_mean, z_log_var, k_samples):\n",
    "    epsilon = random.normal(rand_key, shape=(k_samples,) + z_mean.shape)\n",
    "    return z_mean + jnp.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "fast_sample = jit(sample)\n",
    "\n",
    "decoder_init, decoder_fn = stax.serial(\n",
    "    Dense(hidden_dim), Selu, Dense(hidden_dim), Selu, Dense(hidden_dim), Selu, Dense(input_dim))\n",
    "\n",
    "#initialize the parameters\n",
    "rand_key, key = random.split(rand_key)\n",
    "out_shape, params_dec = decoder_init(rand_key, (-1, latent_dim))\n",
    "\n",
    "params = params_enc + params_dec\n",
    "\n",
    "EPSILON = 1e-6\n",
    "l2 = jit(lambda x, xt: jnp.sum((x - xt)**2, axis=-1))\n",
    "kl = jit(lambda z_mean, z_log_var: - 0.5 * jnp.sum(1 + z_log_var - z_mean ** 2 - jnp.exp(z_log_var), axis=-1))\n",
    "\n",
    "@jit\n",
    "def vae_loss(rand_key, params, x):\n",
    "    latent = jit(encoder_fn)(params[0:7], x)\n",
    "    d = latent.shape[-1]//2\n",
    "    z_mean, z_log_var = latent[:, :d], latent[:,d:]\n",
    "    z_sample = sample(rand_key, z_mean, z_log_var, k_samples)\n",
    "    x_rec = jit(decoder_fn)(params[7:], z_sample)\n",
    "    l2_loss = l2(x, x_rec)\n",
    "    kl_loss = kl(z_mean, z_log_var)\n",
    "    loss = jnp.mean(l2_loss + kl_loss) \n",
    "    return loss\n",
    "    '''\n",
    "    \n",
    "    #log_w_i = jax.lax.stop_gradient(xent_loss + kl_loss)\n",
    "    log_w_i = xent_loss + kl_loss\n",
    "    print(log_w_i.shape)\n",
    "    normalized_w_i = jax.lax.stop_gradient(jax.nn.softmax(log_w_i, axis=-1))\n",
    "    print(normalized_w_i.shape)\n",
    "    weighted_sum = (normalized_w_i * (xent_loss + kl_loss)).sum(axis=0)\n",
    "    print(weighted_sum.shape)\n",
    "    # average over the batch, and sum kl / xent\n",
    "    #loss = jnp.mean(xent_loss) + jnp.mean(kl_loss) \n",
    "    return jnp.mean(weighted_sum) '''\n",
    "\n",
    "# You may run this cell to reinit parameters if needed\n",
    "_, params_enc = encoder_init(rand_key, (-1, input_dim))\n",
    "_, params_dec = decoder_init(rand_key, (-1, latent_dim))\n",
    "params = params_enc + params_dec\n",
    "\n",
    "from jax.experimental import stax, optimizers\n",
    "\n",
    "data_size = X.shape[0]\n",
    "batch_size = 32\n",
    "learning_rate = 0.0003\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(learning_rate)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(key, batch, opt_state):\n",
    "    params = get_params(opt_state)\n",
    "    value_and_grad_fun = jit(jax.value_and_grad(lambda params, x: vae_loss(key, params, x)))\n",
    "    loss, grads = value_and_grad_fun(params, batch)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "data_generator = (X[np.random.choice(X.shape[0], 32)] for _ in range(iters))\n",
    "\n",
    "for epochs in range(iters):\n",
    "    batch = X[i * 32:(i+1)*32]\n",
    "    rand_key, key = random.split(rand_key)\n",
    "    opt_state, loss = update(key, next(data_generator), opt_state)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(opt_state)\n",
    "params_enc = params[0:7]\n",
    "params_dec = params[7:]\n",
    "\n",
    "x_encoded = np.asarray(encoder_fn(params_enc, X[:]))\n",
    "plt.figure(figsize=(7, 7))\n",
    "z_mean, z_log_var = x_encoded[:,0:latent_dim], x_encoded[:,latent_dim:]\n",
    "rand_key, key = random.split(rand_key)\n",
    "z_sample = sample(rand_key, z_mean, z_log_var, k_samples)\n",
    "z_sample = np.reshape(z_sample, (k_samples * x_encoded.shape[0],latent_dim))\n",
    "plt.scatter(z_sample[:, 0], z_sample[:, 1], s=1, c=\"r\")\n",
    "plt.scatter(x_encoded[:, 0], x_encoded[:, 1], s=2, c=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(x_encoded[:,2]/2)), np.mean(np.exp(x_encoded[:,3]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = np.asarray(decoder_fn(params_dec, z_mean))\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=1, c=\"b\")\n",
    "plt.scatter(generated[:, 0], generated[:, 1], s=1, c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_key, key = random.split(rand_key)\n",
    "z = random.normal(key, shape=(1000,latent_dim))\n",
    "generated = np.asarray(decoder_fn(params_dec, z))\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=1, c=\"b\")\n",
    "plt.scatter(generated[:, 0], generated[:, 1], s=2, c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

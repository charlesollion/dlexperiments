{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to open in Colab to access a GPU environment: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/Demos%20VAE%20Jax.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto Encoders using Jax\n",
    "\n",
    "Application to Fashion MNIST dataset, available from tensorflow datasets.\n",
    "Prerequisites:\n",
    "- [tensorflow datasets](https://www.tensorflow.org/datasets) (we won't use tensorflow)\n",
    "- [Jax](https://github.com/google/jax) \n",
    "\n",
    "No GPU required, but the notebook will make use of it if you have one.\n",
    "The dataset considered is Fashion MNIST, which is lightweight and easy to use, and still more interesting than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(0, 18):\n",
    "    plt.subplot(3, 6, i + 1)\n",
    "    plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_standard = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_standard = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "x_train_standard.shape, x_test_standard.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jax mini tutorial\n",
    "\n",
    "[Jax](https://github.com/google/jax) enables to use python and numpy functions with automatic gradient computation, and fast linear algebra through just in time (jit) compilation (see [XLA](https://www.tensorflow.org/xla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use jnp as you would use numpy normally, doing so will enable the autograd and the optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = jnp.zeros((2,3))\n",
    "x = jnp.zeros((2))\n",
    "jnp.dot(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random numbers with jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "rand_key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jax the random state when generating pseudo random numbers is not automatically updated. This is [on purpose](https://github.com/google/jax/blob/master/design_notes/prng.md).\n",
    "For example, run the following example several times using ctrl-enter, noticing that the same random numbers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(rand_key, shape=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually update the state (`rand_key` here), the usual practice is to use the `random.split` function, to get two new random keys. You may then update the state using one of them, and use the other for generation of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2 = random.split(rand_key)\n",
    "\n",
    "#you can update the rand_key\n",
    "rand_key = k1\n",
    "\n",
    "#use k2\n",
    "random.normal(k2, shape=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Encoder\n",
    "\n",
    "The inference model is defined as: \n",
    "\n",
    "$$\\mu_z(\\mathbf{x}), \\sigma_z(\\mathbf{x}) = encoder_{\\phi}(\\mathbf{x})$$\n",
    "$$q_\\phi(z|x) = \\mathcal{N}(z; \\mathbf{\\mu}_z, diag(\\mathbf{\\sigma}_z)) $$\n",
    "\n",
    "We first build $encoder_{\\phi}$ as a MLP with a single hidden layer. Note that $\\mu_z$ and $\\sigma_z$ are $k$-dimensionnal where $k$ is the dimension of the latent space. For visualization purposes we'll choose $k = 2$.\n",
    "\n",
    "The following cells define the parameters $\\phi$ (named here `params_enc`) as several weigth matrices and biases, that we initialize through standard initialization schemes, then the encoder function itself.\n",
    "\n",
    "Note that in practice the output of the encoder network parameterizes $log(\\sigma^2_z(x))$ instead of $\\sigma_z(x)$. Taking the exponential of $log(\\sigma^2_z(x))$ ensures the positivity of the standard deviation from the raw output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax # neural network library\n",
    "from jax.experimental.stax import Dense, Relu, Sigmoid\n",
    "\n",
    "input_dim = x_train_standard.shape[-1]\n",
    "hidden_dim = 128\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_init, encoder_fn = stax.serial(\n",
    "    Dense(hidden_dim), Relu, Dense(latent_dim * 2))\n",
    "\n",
    "#initialize the parameters\n",
    "rand_key, key = random.split(rand_key)\n",
    "out_shape, params_enc = encoder_init(rand_key, (-1, input_dim))\n",
    "\n",
    "print(\"Parameters: (W,b) of first Dense, Relu, (W,b) of second Dense: \")\n",
    "[[p.shape for p in param] for param in params_enc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of `jit` enables just in time compilation which will strongly improve computation speed. The first time you run it, it will compile (and thus be slower), subsequent runs will be faster. You may check this by running the following cell several times with ctrl-enter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time z = jit(encoder_fn)(params_enc, x_train_standard[0:10])\n",
    "%time z = encoder_fn(params_enc, x_train_standard[0:10])\n",
    "print(f\"\\nShape of the output: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparametrization trick\n",
    "\n",
    "We use the reparametrization trick to define a random variable z that is conditioned on the input image x as follows:\n",
    "\n",
    "$$ z \\sim \\mathcal{N}(z, \\mu_z(x), diag(\\sigma_z(x))) $$\n",
    "\n",
    "The reparametrization tricks defines $z$ has follows:\n",
    "\n",
    "$$ z = \\mu_z(x) + \\sigma_z(x) \\cdot \\epsilon$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0, 1) $$\n",
    "\n",
    "This way the dependency to between $z$ and $x$ is deterministic and differentiable. The randomness of $z$ only stems from $\\epsilon$ only for a given $x$.\n",
    "\n",
    "This is illustrated as follows (from Kingma 2015 Neurips):\n",
    "<img src=https://i.stack.imgur.com/TzX3I.png alt=\"Reparametrization Trick\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rand_key, z_mean, z_log_var):\n",
    "    epsilon = random.normal(rand_key, shape=z_mean.shape)\n",
    "    return z_mean + jnp.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "fast_sample = jit(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Decoder\n",
    "\n",
    "We will build the following architecture:\n",
    "\n",
    "$$x^\\prime = decoder_{\\theta}(\\mathbf{z})$$\n",
    "\n",
    "where the decoder is a MLP with a single hidden layer. \n",
    "\n",
    "We first define the parameters $\\theta$ (named here `params_dec`) as several weigth matrices and biases, that we initialize through standard initialization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_init, decoder_fn = stax.serial(\n",
    "    Dense(hidden_dim), Relu, Dense(input_dim), Sigmoid)\n",
    "\n",
    "#initialize the parameters\n",
    "rand_key, key = random.split(rand_key)\n",
    "out_shape, params_dec = decoder_init(rand_key, (-1, latent_dim))\n",
    "\n",
    "params = params_enc + params_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE\n",
    "\n",
    "Let's now the plug the encoder and decoder via the stochastic latent variable $z$ to get the full VAE architecture. The loss function is the negative ELBO of the variational inference problem, the sum of: \n",
    "\n",
    "- crossentropy loss between the input and its reconstruction through the autoencoder. We consider that the output of the decoder parametrizes a Bernoulli distribution. This assumes that the data is valued on $\\{0,1\\}$, which is not true in practice. This assumption will be questioned later.\n",
    "- KL divergence between the distributions of $z$ and the prior distribution (centered gaussian)\n",
    "\n",
    "Intuitively, this corresponds to improving the likelyhood of the model, and making $q_\\phi(z|x)$ match as closely as possible $p(z|x)$\n",
    "\n",
    "note that we will consider all parameters $\\phi \\cup \\theta$ as an argument of the loss function, so that Jax is able to differentiate with regards to each of the parameters: `params = params_enc + params_dec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-6\n",
    "negative_xent = jit(lambda x, y: - jnp.sum(y * jnp.log(x + EPSILON) + (1-y)*jnp.log(1-x + EPSILON), axis=-1))\n",
    "negative_kl = jit(lambda z_mean, z_log_var: - 0.5 * jnp.sum(1 + z_log_var - z_mean ** 2 - jnp.exp(z_log_var), axis=-1))\n",
    "\n",
    "@jit\n",
    "def vae_loss(rand_key, params, x):\n",
    "    # Encoder \n",
    "    latent = jit(encoder_fn)(params[0:3], x)\n",
    "    d = latent.shape[-1]//2\n",
    "    z_mean, z_log_var = latent[:, :d], latent[:,d:]\n",
    "    \n",
    "    # Sample\n",
    "    z_sample = fast_sample(rand_key, z_mean, z_log_var)\n",
    "    \n",
    "    # Decoder\n",
    "    x_rec = jit(decoder_fn)(params[3:], z_sample)\n",
    "    \n",
    "    xent_loss = negative_xent(x_rec, x)\n",
    "    kl_loss = negative_kl(z_mean, z_log_var)\n",
    "    \n",
    "    # average over the batch, and sum kl / xent\n",
    "    negative_elbo = jnp.mean(xent_loss) + jnp.mean(kl_loss) \n",
    "    return negative_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time vae_loss(rand_key, params, x_train_standard[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff in Jax\n",
    "\n",
    "The `grad` operator in Jax takes a loss function, and computes automatically all differentiations, returning the gradient of the loss function with regards to the parameters, for a given input datapoint. Note the following syntax:\n",
    "\n",
    "If `loss` is a function of parameters and input datapoint `loss = f(param, x)`, `grad(loss)` returns a new function, which takes the same input as `loss`: `grad(loss) = f_grad(param, x)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "rand_key, key = random.split(rand_key)\n",
    "vae_loss_rand = partial(vae_loss, key)\n",
    "\n",
    "grads = grad(vae_loss_rand)(params, x_train_standard[0:1])\n",
    "\n",
    "print(\"Shapes of all gradients of the loss with regards to each parameter, for a batch of 1 inputs: \\n\")\n",
    "names = [\"[enc] (W,b) of first Dense\", \"[enc] Relu\", \"[enc] W,b) of second Dense\",\n",
    "         \"[dec] (W,b) of first Dense\", \"[dec] Relu\", \"[dec] (W,b) of second Dense\", \"[dec] Sigmoid\"]\n",
    "\n",
    "print(\"\\n\".join([name + \" : \"+ \" \".join([str(p.shape) for p in param]) for name, param in zip(names, grads)]))\n",
    "\n",
    "print(\"\\nImage, Gradients of the first dense, gradient of the last dense, summed for each pixel\")\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(x_train[0], cmap=\"gray\");\n",
    "plt.axis(\"off\");\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(grads[0][0].sum(axis=-1).reshape(28,28));\n",
    "plt.axis(\"off\");\n",
    "cb = plt.colorbar()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(grads[5][0].sum(axis=0).reshape(28,28));\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the VAE\n",
    "\n",
    "The following cells:\n",
    "    - reinitialize parameters\n",
    "    - initialize an Adam optimizer\n",
    "    - run a batch training over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may run this cell to reinit parameters if needed\n",
    "_, params_enc = encoder_init(rand_key, (-1, input_dim))\n",
    "_, params_dec = decoder_init(rand_key, (-1, latent_dim))\n",
    "params = params_enc + params_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax, optimizers\n",
    "\n",
    "data_size = x_train_standard.shape[0]\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(learning_rate)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(key, batch, opt_state):\n",
    "    params = get_params(opt_state)\n",
    "    value_and_grad_fun = jit(jax.value_and_grad(lambda params, x: vae_loss(key, params, x)))\n",
    "    loss, grads = value_and_grad_fun(params, batch)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(5):\n",
    "    # Shuffle the dataset\n",
    "    rand_key, key = random.split(rand_key)\n",
    "    permutation = random.permutation(key, data_size)\n",
    "    for i in range(data_size // 32 - 1):\n",
    "        batch = x_train_standard[permutation[i * 32:(i+1)*32]]\n",
    "        rand_key, key = random.split(rand_key)\n",
    "        opt_state, loss = update(key, batch, opt_state)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "\n",
    "We can test the decoder with random samples from the prior distribution of $z$. You may run the following cell several times through ctrl-enter.\n",
    "\n",
    "Note that after the decoder pass, instead of generating a samples from the factorized Bernoulli observation model (which would generate values in $\\{0,1\\}^{28 \\times 28}$, we rather interpret the parameter of the distribution which takes values in $]0,1[^{28 \\times 28}$ as the pixel value directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_key, key = random.split(key)\n",
    "params = get_params(opt_state)\n",
    "params_dec = params[3:]\n",
    "z = random.normal(key, shape=(1,latent_dim))\n",
    "generated = decoder_fn(params_dec, z)\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D plot of the image classes in the latent space\n",
    "\n",
    "We can also use the encoder to set the visualize the distribution of the test set in the 2D latent space of the VAE model. In the following the colors show the true class labels from the test samples.\n",
    "\n",
    "Note that the VAE is an unsupervised model: it did not use any label information during training. However we can observe that the 2D latent space is largely structured around the categories of images used in the training set.\n",
    "\n",
    "We will also compare the latent space with a PCA decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_labels = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", \n",
    "                5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_train_standard)\n",
    "encoded_pca_x=pca.transform(x_test_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder_fn(params[0:3], x_test_standard)\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test,\n",
    "            cmap=plt.cm.tab10, s=5)\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks(list(id_to_labels.keys()))\n",
    "cb.set_ticklabels(list(id_to_labels.values()))\n",
    "cb.update_ticks()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(encoded_pca_x[:, 0], encoded_pca_x[:, 1], c=y_test,\n",
    "            cmap=plt.cm.tab10, s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does the model reconstruct test examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_samples = decoder_fn(params[3:], x_test_encoded[0:6,0:2])\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(0, 6):\n",
    "    plt.subplot(2, 6, i + 1)\n",
    "    plt.imshow(x_test[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "for i in range(6, 12):\n",
    "    plt.subplot(2, 6, i + 1)\n",
    "    img = reconstructed_samples[i - 6].reshape(28, 28)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D panel view of samples from the VAE manifold\n",
    "\n",
    "The following linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian to produce values of the latent variables z. This makes it possible to use a square arangement of panels that spans the gaussian prior of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15  # figure with 15x15 panels\n",
    "image_size = 28\n",
    "figure = np.zeros((image_size * n, image_size * n))\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n)).astype(np.float32)\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n)).astype(np.float32)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder_fn(params[3:], z_sample)\n",
    "        image = x_decoded[0].reshape(image_size, image_size)\n",
    "        figure[i * image_size: (i + 1) * image_size,\n",
    "               j * image_size: (j + 1) * image_size] = image\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VAE import VanillaVAE, IWAE, VectorQuantizedVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example: 8 gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eightgaussian(n_points):\n",
    "    \"\"\"\n",
    "     Returns the eight gaussian dataset.\n",
    "    \"\"\"\n",
    "    n = np.random.randint(0,8, n_points)\n",
    "    noisex = np.random.normal(size=(n_points)) * 0.2\n",
    "    noisey = np.random.normal(size=(n_points)) * 0.2\n",
    "    x_centers,y_centers = [np.cos(n* np.pi/4.0) * 5 + noisex, np.sin(n* np.pi/4.0) * 5 + noisey]\n",
    "    return np.vstack((x_centers,y_centers)).T\n",
    "            \n",
    "X = eightgaussian(10000)\n",
    "X_test = eightgaussian(5000)\n",
    "X.shape\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0], X[:,1], s=1);\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import make_dataloaders, save_model, DotDict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = torch.utils.data.DataLoader(X, batch_size=32, collate_fn=lambda x:torch.Tensor(x))\n",
    "val_loader = torch.utils.data.DataLoader(X_test, batch_size=32, collate_fn=lambda x:torch.Tensor(x))\n",
    "\n",
    "clipping_value=0.1\n",
    "\n",
    "def train(model, epochs):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    model.to(device)\n",
    "    optim = model.get_optimizer()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            loss, x_hat, z, BCE = model.step(batch)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "    print(f\"{model.description} epoch: {epoch} loss:{losses[-1]:.2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vq(model, dims=[0,1]):\n",
    "    \n",
    "    x_tilde, z_e_x, z_q_x = model.forward(torch.Tensor(X_test[:300]))\n",
    "    rec_x = x_tilde.detach().cpu().numpy()\n",
    "    z_e_x=z_e_x.detach().cpu().numpy()\n",
    "    z_q_x=z_q_x.detach().cpu().numpy()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(X_test[:300, 0], X_test[:300, 1], s=2, c=\"y\", alpha=0.5, label=\"x\")\n",
    "    plt.scatter(z_e_x[:, dims[0]], z_e_x[:, dims[1]], s=2, c=\"g\", alpha=0.5, label=\"z_e_x\")\n",
    "    plt.scatter(z_q_x[:, dims[0]], z_q_x[:, dims[1]], s=100, edgecolors=\"b\", label=\"z_q_x\", alpha=1, marker=\"o\", facecolors='none')\n",
    "    plt.scatter(rec_x[:, 0], rec_x[:, 1], s=100, edgecolors=\"r\", alpha=1.0,label=\"reconstructed\", marker=\"o\", facecolors='none')\n",
    "    ax = fig.axes[0]\n",
    "    style = dict(size=20, color='gray')\n",
    "    embs = model.codebook.embedding.weight.data\n",
    "    embs_dec = model.net.decode(embs).data\n",
    "    plt.scatter(embs[:, dims[0]], embs[:, dims[1]], s=20, c=\"b\", label=\"embeddings\", marker=\"x\")\n",
    "    plt.scatter(embs_dec[:, dims[0]], embs_dec[:, dims[1]], s=20, c=\"r\", label=\"embeddings_dec\", marker=\"x\")\n",
    "    for i in range(embs.shape[0]):\n",
    "        ax.text(embs[i,0], embs[i,1], f\"{i}\", **style)\n",
    "        ax.text(embs_dec[i,0], embs_dec[i,1], f\"{i}\", **style)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tilde, z_e_x, z_q_x = vqvae_toy.forward(torch.Tensor(X_test[:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_toy = VectorQuantizedVAE(latent_dim = 2, K=8, output_dim=2, beta=0.1, gumbel=True, hidden_dim=40, num_hidden=1, archi=\"basic\", data_type=\"continuous\")\n",
    "train(vqvae_toy, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_vq(vqvae_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQ-VAE with Gumbel-Softmax on FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = make_dataloaders(\"fashionmnist\", 32, device, subset=32*32, binarize=False, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae = VectorQuantizedVAE(latent_dim = 64, gumbel=True, tau=1., alpha=1., beta=1., K=128, num_hidden=0, output_dim=784, hidden_dim=128, normalize=True, activation=\"relu\", archi=\"convMnist\", data_type=\"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(vqvae, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = vqvae.encode(batch)\n",
    "rec = vqvae.decode(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 2))\n",
    "for idx in range(16):\n",
    "    plt.subplot(2, 16, idx+1)\n",
    "    plt.axis('off');\n",
    "    plt.imshow(batch[idx].data.view(28,28).numpy(), cmap=\"gray\");\n",
    "    plt.subplot(2, 16, idx+17)\n",
    "    plt.axis('off');\n",
    "    plt.imshow(rec[idx].data.view(28,28).numpy(), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = vqvae.encode(batch)\n",
    "l = latents[7:8]\n",
    "print(l[0])\n",
    "plt.imshow(vqvae.decode(l)[0].data.view(28,28).numpy(), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5fU0bcRFM7B"
   },
   "source": [
    "# Crash Course on Bayesian Deep Learning\n",
    "\n",
    "Click to run on colab (if you're not already there): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/BayesianDeepWine.ipynb) \n",
    "\n",
    "This session aims at understanding and implementing basic Bayesian Deep Learning models, as described in [Bayes by Backprop](https://arxiv.org/abs/1505.05424), and a short comparison with [Monte Carlo Dropout](  https://arxiv.org/abs/1506.02142).\n",
    "\n",
    "**What this is not about:**\n",
    "\n",
    "- answering questions about a dataset\n",
    "- Deep Bayesian Learning: *What*, *Why*\n",
    "\n",
    "**What this is about:**\n",
    "\n",
    "- Deep Bayesian Learning: *How*\n",
    "- trying to stick to classic deep learning frameworks and practice\n",
    "- understanding basic building blocks\n",
    "\n",
    "The notebook itself is inspired from [Khalid Salama's Keras tutorial on Bayesian Deep Learning](https://keras.io/examples/keras_recipes/bayesian_neural_networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning frameworks introduction\n",
    "\n",
    "Our goal is to use existing frameworks such as Pytorch or Tensorflow. This usually implies:\n",
    "\n",
    "- We will use Stochastic Gradient Descent (SGD) or a variant\n",
    "- We will then train on random mini-batches of data (data may be large)\n",
    "- We want to take advantage of automatic differentiation and not compute gradients ourselves: everything must be **differentiable**\n",
    "- We want to benefit from parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjW7a270FM7G"
   },
   "source": [
    "## Installation\n",
    "\n",
    "In this notebook, basic probabilistic Bayesian neural networks are built, with a focus on practical implementation. We consider both of the most populat deep learning frameworks: Tensorflow (and Keras) or Pytorch. Feel free to use your favorite.\n",
    "\n",
    "For Tensorflow (2.3 or higher), [TensorFlow Probability](https://www.tensorflow.org/probability) library is used which is compatible with Keras API. If you're outside of Colab, you may install it with pip:\n",
    "\n",
    "```python\n",
    "pip install tensorflow-probability\n",
    "```\n",
    "\n",
    "For pytorch, the built-in `torch.distributions` will be used:\n",
    "\n",
    "```python\n",
    "pip install pytorch\n",
    "```\n",
    "\n",
    "Outside of Colab, you can install [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wine_quality) using the following command:\n",
    "\n",
    "```python\n",
    "pip install tensorflow-datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJcfG-w_FM7H"
   },
   "source": [
    "## The dataset\n",
    "\n",
    "We use the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality)\n",
    "dataset, which is available in the .\n",
    "We use the red wine subset, which contains 4,898 examples.\n",
    "The dataset has 11numerical physicochemical features of the wine, and the task\n",
    "is to predict the wine quality, which is a score between 0 and 10.\n",
    "\n",
    "While the experts gave integer scores, we will first consider them as continuous values between 0 and 10 and treat this as a regression task, for simplicity and easy interpretation of confidence intervals. \n",
    "\n",
    "We could consider instead consider a classification task, add an observation noise, or many different things here, but it's not the point of this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oab6Gl17FM7I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Benefits of Keras/Tensorflow\n",
    "# + no management of CPU/GPU\n",
    "# + once the model is defined, very easy to train\n",
    "# - quite verbose code to define models\n",
    "# - a lot is hidden, difficult to track bugs and what's really going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvwzBZvRkT4j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Benefits of Pytorch\n",
    "# + easier to debug and to control\n",
    "# - manage GPU/CPU (but it's easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYDrwmZTFM7K"
   },
   "source": [
    "### Create training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBLXTxydFM7L"
   },
   "outputs": [],
   "source": [
    "FEATURE_NAMES = [\n",
    "    \"fixed acidity\", \"volatile acidity\", \"citric acid\",\n",
    "    \"residual sugar\", \"chlorides\", \"free sulfur dioxide\",\n",
    "    \"total sulfur dioxide\", \"density\", \"pH\",\n",
    "    \"sulphates\", \"alcohol\",\n",
    "]\n",
    "\n",
    "def process_data(x,y):\n",
    "    x_transform = tf.stack([tf.cast(x[f], tf.float32) for f in FEATURE_NAMES], 0)\n",
    "    return x_transform, tf.cast(y, tf.float32) # cast y (integer) as a float\n",
    "\n",
    "def get_train_and_test_splits(train_size, batch_size=1):\n",
    "    dataset = (\n",
    "        tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
    "        .map(process_data)\n",
    "        .cache()\n",
    "    )\n",
    "    # We shuffle with a buffer the same size as the dataset.\n",
    "    train_dataset = dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
    "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06fXwfRCFM7Q"
   },
   "source": [
    "Let's split the wine dataset into training and test sets, with 85% and 15% of\n",
    "the examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpWvkoziFM7R"
   },
   "outputs": [],
   "source": [
    "dataset_size = 4898\n",
    "batch_size = 256\n",
    "train_size = int(dataset_size * 0.85)\n",
    "M = int(train_size / batch_size)\n",
    "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yimLXWxgm61V"
   },
   "source": [
    "### Pytorch conversion\n",
    "\n",
    "We can transform the tensorflow dataset into a pytorch one. This is not a very good practice, but is doable as the dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQXNftUguZbm"
   },
   "outputs": [],
   "source": [
    "dataset = tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\").map(process_data)\n",
    "\n",
    "def process_data_torch(x,y):\n",
    "    x_transform = torch.tensor(x.numpy(), device=device)\n",
    "    y_transform = torch.tensor(y.numpy(), device=device)\n",
    "    return (x_transform, y_transform)\n",
    "\n",
    "data_train = list(map(lambda x: process_data_torch(*x), dataset.take(train_size)))\n",
    "data_test = list(map(lambda x: process_data_torch(*x), dataset.skip(train_size)))\n",
    "torch_trainloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size)\n",
    "torch_testloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUEGoFbJFM7P"
   },
   "source": [
    "## 1) Standard neural network\n",
    "\n",
    "We create a standard deterministic neural network model as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP2WOEc2FM7M"
   },
   "source": [
    "### Tensorflow / Keras version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO601s3aFM7Q"
   },
   "outputs": [],
   "source": [
    "def create_baseline_model(input_dim, hidden_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Hidden layer with deterministic weights using a Dense layer, \n",
    "    # and non linear activation function\n",
    "    features = layers.Dense(hidden_dim, activation=\"sigmoid\")(inputs)\n",
    "\n",
    "    # The output is deterministic: a single point estimate.\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obqZ09QKFM7N"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "def run_experiment(model, loss, epochs, train_dataset, test_dataset):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
    "    _, rmse = model.evaluate(train_dataset, verbose=0)\n",
    "    print(f\"Train RMSE: {round(rmse, 3)}\")\n",
    "\n",
    "    _, rmse = model.evaluate(test_dataset, verbose=0)\n",
    "    print(f\"Test RMSE: {round(rmse, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = create_baseline_model(11, 32)\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzwEh59mFM7R"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "mse_loss = keras.losses.MeanSquaredError()\n",
    "run_experiment(baseline_model, mse_loss, num_epochs, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYbKzM2rFM7R"
   },
   "source": [
    "We take a sample from the test set use the model to obtain predictions for them.\n",
    "Note that since the baseline model is deterministic, we get a single a\n",
    "*point estimate* prediction for each test example, with no information about the\n",
    "uncertainty of the model nor the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1NmyJE_FM7T"
   },
   "outputs": [],
   "source": [
    "samples = 10\n",
    "examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(samples))[\n",
    "    0\n",
    "]\n",
    "\n",
    "predicted = baseline_model(examples).numpy()\n",
    "for idx in range(samples):\n",
    "    print(f\"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpwIN76Q1MPb"
   },
   "source": [
    "### Pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyqun6Ch2IaQ"
   },
   "outputs": [],
   "source": [
    "class BaselineTorchModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(BaselineTorchModel, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.act = torch.sigmoid\n",
    "        #self.batch_norm = torch.nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #inputs = self.batch_norm(inputs)\n",
    "        h = self.hidden_layer(inputs)\n",
    "        h = self.act(h)\n",
    "        output = self.out_layer(h)\n",
    "        \n",
    "        # we add a dummy output, a placeholder for future experiments\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-yBAJje3LEd"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_experiment_torch(model, loss, num_epochs, train_dataloader, test_dataloader):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        for x, y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss_value = loss(model(x), y)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "    errors = []\n",
    "    for x,y in test_dataloader:\n",
    "        yhat, _ = model(x)\n",
    "        errors.append((torch.squeeze(yhat.detach()).cpu().numpy() - y.detach().cpu().numpy())**2)\n",
    "  \n",
    "    rmse = np.sqrt(np.mean(np.concatenate(errors, axis=None)))\n",
    "    print(f\"Test RMSE: {round(rmse, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smdJBd6q287-"
   },
   "outputs": [],
   "source": [
    "baseline_torch_model = BaselineTorchModel(11, 32).to(device)\n",
    "baseline_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.numel() for p in baseline_torch_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smdJBd6q287-"
   },
   "outputs": [],
   "source": [
    "def simple_mse_loss(model_outputs, y_true):\n",
    "    yhat, _ = model_outputs\n",
    "    yhat = torch.squeeze(yhat)\n",
    "    return torch.nn.MSELoss()(yhat, y_true) \n",
    "\n",
    "run_experiment_torch(baseline_torch_model, \n",
    "                     simple_mse_loss, \n",
    "                     50, torch_trainloader, torch_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KlQB2lY8UdN"
   },
   "outputs": [],
   "source": [
    "samples = 10\n",
    "examples_torch, targets_torch = next(iter(torch_testloader))\n",
    "predicted, _ = baseline_torch_model(examples_torch[:samples])\n",
    "predicted = predicted.detach().cpu().numpy()\n",
    "for idx in range(samples):\n",
    "    print(f\"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets_torch[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWS8r4qKmf5z"
   },
   "source": [
    "## 2) A simple Bayesian Neural Network\n",
    "\n",
    "Our objective is to build a single layer Bayesian Neural Network using Tensorflow or Pytorch. \n",
    "\n",
    "nstead of learning specific weight *values* in the\n",
    "neural network, we will instead learns weight *distributions*, from which we can sample to produce a weight value, and then the output given an input.\n",
    "\n",
    "We need to define prior and the posterior distributions of these weights, we will start by having a unit Gaussian prior, and a diagonal covariance multivariate Gaussian posterior.\n",
    "\n",
    "From the [Bayes by Backprop](https://arxiv.org/abs/1505.05424) paper, we have the following algorithm:\n",
    "\n",
    "1. Sample $\\epsilon ∼ N(0, I)$.\n",
    "2. Let $w = µ + log(1 + exp(ρ)) ◦ \\epsilon$.\n",
    "3. Let $θ = (µ, ρ)$.\n",
    "4. Let $f(w, θ) = log q(w|θ) − log P(w)P(D|w)$.\n",
    "5. Compute gradients with respect to everything\n",
    "\n",
    "Note that the $∂f(w,θ) ∂w$ term of the gradients for the mean and standard deviation are shared and are exactly the gradients found by the usual backpropagation algorithm on a neural network. Thus, remarkably, to learn both the mean and the standard deviation we must simply calculate the usual gradients found by backpropagation.\n",
    "\n",
    "The cost function is seperated in two terms, one which corresponds to the standard mse loss as before, and the other to a KL divergence between the posterior q and the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58W02YLcD8vs"
   },
   "source": [
    "### Tensorflow / Keras version\n",
    "\n",
    "We will first consider having a single layer that has distribution over the weights. Wa can derive the Keras Layer class to build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ask5zOYVMZjs"
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.distributions import kullback_leibler\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "class LinearVariational(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "               units,\n",
    "               kl_weight=1.0,\n",
    "               activation=None):\n",
    "        super(LinearVariational, self).__init__()\n",
    "        self.units = int(units)\n",
    "        self.kl_weight = kl_weight\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    \n",
    "    def _make_prior(self, n):\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(n), \n",
    "                                                 scale_diag=tf.ones(n))\n",
    "        \n",
    "    def _make_posterior(self, dim):\n",
    "        # A bit of boilerplate to define Keras Layers with the parameters\n",
    "        trainable_normal = tf.keras.models.Sequential([\n",
    "            tfp.layers.VariableLayer(\n",
    "              shape=[dim, 2], #first is for mu, second for rho\n",
    "              dtype=tf.float64,\n",
    "              initializer=tfp.layers.BlockwiseInitializer([\n",
    "                  'zeros', # initialize mu to 0\n",
    "                  tf.keras.initializers.Constant(np.log(np.expm1(1.))), # initialize rho s.t. softplus(rho)=1\n",
    "              ], sizes=[1, 1])),\n",
    "            tfp.layers.DistributionLambda(lambda t: tfd.MultivariateNormalDiag(loc=t[..., 0], \n",
    "                                                                             scale_diag=tf.math.softplus(t[..., 1])))\n",
    "            ])\n",
    "\n",
    "        return trainable_normal\n",
    "      \n",
    "      \n",
    "    def build(self, input_shape):\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        self.posterior = self._make_posterior(last_dim * self.units)\n",
    "        self.prior = self._make_prior(last_dim * self.units)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = self.posterior(0.) # input not used, but a keras model takes inputs\n",
    "        p = self.prior\n",
    "        \n",
    "        kl = kullback_leibler.kl_divergence(q, p)\n",
    "        self.add_loss(tf.reduce_sum(kl) * self.kl_weight)\n",
    "        \n",
    "        w = q.sample() # differentiable because MultivariateNormalDiag \n",
    "                       # implicitely uses reparametrization trick\n",
    "\n",
    "        # Otherwise, we could have coded the reparametrization trick\n",
    "        # directly from mu and rho as follows:\n",
    "        # epsilon = tf.random.normal(mu.shape, 0, 1, tf.float32)\n",
    "        # w = mu + tf.math.softplus(rho) * epsilon\n",
    "\n",
    "        w = tf.reshape(w, shape=[-1, self.units])\n",
    "        outputs = tf.matmul(inputs, w)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtihc700MyZN"
   },
   "outputs": [],
   "source": [
    "def create_bnn_model(input_dim, hidden_dim, kl_weight=1.):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    features = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    # Create our stochastic layer which has a distribution over weights\n",
    "    z = LinearVariational(hidden_dim, kl_weight=kl_weight)(features)\n",
    "\n",
    "    # The output is deterministic: a single point estimate.\n",
    "    output = layers.Dense(units=1)(z)\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_model_small = create_bnn_model(11, 32, kl_weight = 1. / train_size)\n",
    "bnn_model_small.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are now more trainable parameters, even though we don't have a bias for the hidden layer.\n",
    "\n",
    "The KL weight, the authors of the original paper propose either $1/M$ with $M$ the number of minibatches in the dataset for each epoch, of $\\frac{2^{M-i}}{2^M-1}$ where $i$ is the batch index. In practice here it was chosen as an hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Li5dd8dLNdAK"
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "run_experiment(bnn_model_small, mse_loss, num_epochs, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy0J3DRSFM7V"
   },
   "outputs": [],
   "source": [
    "def compute_predictions(model, iterations=100):\n",
    "    predicted = []\n",
    "    for _ in range(iterations):\n",
    "        predicted.append(model(examples).numpy())\n",
    "    predicted = np.concatenate(predicted, axis=1)\n",
    "    return predicted\n",
    "\n",
    "def display_predictions(predictions, targets):\n",
    "    prediction_mean = np.mean(predictions, axis=1).tolist()\n",
    "    prediction_min = np.min(predictions, axis=1).tolist()\n",
    "    prediction_max = np.max(predictions, axis=1).tolist()\n",
    "    prediction_range = (np.max(predictions, axis=1) - np.min(predictions, axis=1)).tolist()\n",
    "\n",
    "    for idx in range(samples):\n",
    "        print(\n",
    "            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n",
    "            f\"min: {round(prediction_min[idx], 2)}, \"\n",
    "            f\"max: {round(prediction_max[idx], 2)}, \"\n",
    "            f\"range: {round(prediction_range[idx], 2)} - \"\n",
    "            f\"Actual: {targets[idx]}\"\n",
    "        )\n",
    "\n",
    "predictions = compute_predictions(bnn_model_small)\n",
    "display_predictions(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDaE3yXxD27U"
   },
   "source": [
    "### Pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TY8J6QW8vHCG"
   },
   "outputs": [],
   "source": [
    "class BnnTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation=None):\n",
    "        super(BnnTorch, self).__init__()\n",
    "        n = input_dim * hidden_dim\n",
    "        self.mu = nn.Parameter(torch.zeros((n), dtype=torch.float32))\n",
    "        self.rho  = nn.Parameter(torch.log(torch.expm1(torch.ones((n), dtype=torch.float32))))\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.act = activation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prior = torch.distributions.Normal(loc=torch.zeros((n), device=device, dtype=torch.float32),\n",
    "                                                scale=torch.ones((n), device=device, dtype=torch.float32))\n",
    "        self.kl_func = torch.distributions.kl.kl_divergence\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(input_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.batch_norm(inputs)\n",
    "        q = torch.distributions.Normal(loc=self.mu, \n",
    "                                       scale=torch.log(1.+torch.exp(self.rho)))\n",
    "        \n",
    "        kl = torch.sum(self.kl_func(q, self.prior))\n",
    "        # we use q.rsample() which uses the reparametrization trick instead of \n",
    "        # q.sample() which breaks the auto-differentation path\n",
    "        w = q.rsample() \n",
    "        w = w.reshape((-1, self.hidden_dim))\n",
    "        h = inputs @ w\n",
    "        if self.act is not None:\n",
    "            h = self.act(h)\n",
    "        output = self.out_layer(h)\n",
    "        return output, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9CErey5vW84"
   },
   "outputs": [],
   "source": [
    "bnn_torch = BnnTorch(11, 32).to(device)\n",
    "bnn_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9CErey5vW84"
   },
   "outputs": [],
   "source": [
    "kl_weight = 1. / train_size\n",
    "\n",
    "def mse_kl_loss(model_outputs, y_true):\n",
    "    yhat, kl = model_outputs\n",
    "    yhat = torch.squeeze(yhat)\n",
    "    mse = torch.nn.MSELoss()(yhat, y_true)\n",
    "    return mse + kl * kl_weight\n",
    "\n",
    "run_experiment_torch(bnn_torch, \n",
    "                     mse_kl_loss, \n",
    "                     200, torch_trainloader, torch_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuxe0T3fzVTJ"
   },
   "outputs": [],
   "source": [
    "def compute_predictions_torch(model, iterations=100):\n",
    "    predicted = []\n",
    "    model.eval()\n",
    "    for _ in range(iterations):\n",
    "        preds, _ = model(examples_torch)\n",
    "        predicted.append(preds.detach().cpu().numpy())\n",
    "    predicted = np.concatenate(predicted, axis=1)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLqsO83iy2Dl"
   },
   "outputs": [],
   "source": [
    "predictions = compute_predictions_torch(bnn_torch)\n",
    "display_predictions(predictions, targets_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCJ7r7n8FM7T"
   },
   "source": [
    "## 3) Full Bayesian neural network using existing modules\n",
    "\n",
    "In order to build more complex networks, we can use the `tfp.layers.DenseVariational` layer instead of our custom layers. For this layer, the Tensorflow [help page](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational) show us that we need to define two functions, `make_posterior_fn`, `make_prior_fn` which return `tfd.Distribution`, and parametrized as keras models.\n",
    "\n",
    "We will also parametrize more complex posterior distributions, using `tfp.distributions.MultivariateNormalTriL` with more complex covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0YM4NmNFM7U"
   },
   "outputs": [],
   "source": [
    "# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n",
    "# Note that, in this example, the we prior distribution is not trainable,\n",
    "# as we fix its parameters.\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.DistributionLambda(\n",
    "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return prior_model\n",
    "\n",
    "\n",
    "# Define variational posterior weight distribution as multivariate Gaussian.\n",
    "# Note that the learnable parameters for this distribution are the means,\n",
    "# variances, and covariances.\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.VariableLayer(\n",
    "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "            ),\n",
    "            tfp.layers.MultivariateNormalTriL(n),\n",
    "        ]\n",
    "    )\n",
    "    return posterior_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6XyhK98FM7X"
   },
   "outputs": [],
   "source": [
    "def create_full_bnn_model(input_dim, hidden_units, kl_weight):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    features = layers.BatchNormalization()(inputs)\n",
    "\n",
    "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
    "    for units in hidden_units:\n",
    "        features = tfp.layers.DenseVariational(\n",
    "            units=units,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=kl_weight,\n",
    "            activation=\"sigmoid\",\n",
    "        )(features)\n",
    "\n",
    "    outputs = tfp.layers.DenseVariational(\n",
    "            units=1,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=kl_weight,\n",
    "    )(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-kynJJJ_COA"
   },
   "source": [
    "\n",
    "In practice, [flipout](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseFlipout) layers (`tfp.layers.DenseFlipout`) and [local reparametrization](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseLocalReparameterization) layers (`tfp.layers.DenseLocalReparameterization`) are also used. You may also find convolutional versions of these, or RNN versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZ0BqFuMFM7W"
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "bnn_model_full = create_full_bnn_model(11, [8, 8], 1/train_size)\n",
    "run_experiment(bnn_model_full, negative_loglikelihood, num_epochs, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZwvhkEJHX4p"
   },
   "outputs": [],
   "source": [
    "predictions = compute_predictions(bnn_model_full)\n",
    "display_predictions(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53tmcKY_FM7X"
   },
   "source": [
    "## 4)Bayesian Neural Network parametrizing a distribution\n",
    "\n",
    "So far, we have distributions over weights, but single point estimates for each input (once we sample these weights).\n",
    "\n",
    "We can also model an output distribution, rather than a single point estimate. We will change the last layer to become a parametrization of a Gaussian as the output. This should be related to *aleatoric uncertainty*, irreducible noise in the data, capturing the stochastic nature of the process generating the data.\n",
    "\n",
    "We then use the negative loglikelihood of this distribution as our loss function, instead of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl9eMY2OVAtP"
   },
   "outputs": [],
   "source": [
    "def create_probabilistic_bnn_model(input_dim, hidden_units, kl_weight):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    features = layers.BatchNormalization()(inputs)\n",
    "\n",
    "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
    "    for units in hidden_units:\n",
    "        features = tfp.layers.DenseVariational(\n",
    "            units=units,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=kl_weight,\n",
    "            activation=\"sigmoid\",\n",
    "        )(features)\n",
    "\n",
    "    distribution_params = layers.Dense(units=2)(features)\n",
    "    outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY-7WZu1VFJ6"
   },
   "outputs": [],
   "source": [
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)\n",
    "\n",
    "num_epochs = 500\n",
    "bnn_model_probabilistic = create_probabilistic_bnn_model(11, [8, 8], 1/train_size)\n",
    "run_experiment(bnn_model_probabilistic, negative_loglikelihood, num_epochs, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PvyMNK5FM7Y"
   },
   "outputs": [],
   "source": [
    "prediction_distribution = bnn_model_probabilistic(examples)\n",
    "prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
    "prediction_stdv = prediction_distribution.stddev().numpy()\n",
    "\n",
    "# The 95% CI is computed as mean ± (1.96 * stdv)\n",
    "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
    "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
    "prediction_stdv = prediction_stdv.tolist()\n",
    "\n",
    "for idx in range(samples):\n",
    "    print(\n",
    "        f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
    "        f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
    "        f\"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]\"\n",
    "        f\" - Actual: {targets[idx]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibgd6w0OWzoj"
   },
   "source": [
    "## 5) MC Dropout\n",
    "\n",
    "We will now consider a simpler approach, as described in the [Monte Carlo Dropout](  https://arxiv.org/abs/1506.02142) paper. The implementation is extremely simple to people already familiar with dropout: we just need to keep dropout active at test time by forcing `training=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJvAfmjUXWU3"
   },
   "outputs": [],
   "source": [
    "def create_mcdropout_model(input_dim, hidden_dim, p):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Create a hidden layer with deterministic weights using the Dense layer.\n",
    "    features = layers.Dense(hidden_dim, activation=\"sigmoid\")(inputs)\n",
    "    features = tf.keras.layers.Dropout(p)(features, training=True)\n",
    "    # The output is deterministic: a single point estimate.\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK7xtpLvYEWf"
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "mse_loss = keras.losses.MeanSquaredError()\n",
    "mc_model = create_mcdropout_model(11, 32, 0.5)\n",
    "run_experiment(mc_model, mse_loss, num_epochs, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVUb8pQvZEVO"
   },
   "outputs": [],
   "source": [
    "predictions = compute_predictions(mc_model)\n",
    "display_predictions(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pl6D20rbCJSe"
   },
   "source": [
    "## 6) Convolutional model\n",
    "\n",
    "This bonus section showcases a larger bayesian neural network with convolutional layers. It is extracted from a [github repository](https://github.com/kumar-shridhar/PyTorch-BayesianCNN). It is trained on CIFAR10\n",
    "\n",
    "All weights, including the convolutionnal kernels are sampled from a weight distribution:\n",
    "![](https://github.com/kumar-shridhar/PyTorch-BayesianCNN/raw/master/experiments/figures/CNNwithdist_git.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMY-6319CMfw"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kumar-shridhar/PyTorch-BayesianCNN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSosnOyDCqj7"
   },
   "outputs": [],
   "source": [
    "%cd PyTorch-BayesianCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITrWrC--LyxO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import data\n",
    "import utils\n",
    "import metrics\n",
    "import config_bayesian as cfg\n",
    "from models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
    "\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LVK8mphL64t"
   },
   "outputs": [],
   "source": [
    "trainset, testset, inputs, outputs = data.getDataset(\"CIFAR10\")\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "        trainset, testset, cfg.valid_size, 32, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a pretrained model (could be more trained!). Warning, it might only work on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/model_3conv3fc_lrt_softplus.pt?raw=true -O model_3conv3fc_lrt_softplus.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7R9SMK0K8t4"
   },
   "outputs": [],
   "source": [
    "weights_path = \"model_3conv3fc_lrt_softplus.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxyvHauVNHw9"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(weights_path, map_location=device)\n",
    "net = BBB3Conv3FC(outputs, inputs, cfg.priors, cfg.layer_type, cfg.activation_type).to(device)\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6FeZ63eO7i8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def validate_model(net, validloader, num_ens=1):\n",
    "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
    "    net.eval()\n",
    "    accs = []\n",
    "\n",
    "    for i, (inputs, labels) in tqdm(enumerate(validloader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
    "        for j in range(num_ens):\n",
    "            net_out, _ = net(inputs)\n",
    "            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
    "\n",
    "        log_outputs = utils.logmeanexp(outputs, dim=2)\n",
    "        accs.append(metrics.acc(log_outputs, labels))\n",
    "\n",
    "    return np.mean(accs)\n",
    "\n",
    "validate_model(net, test_loader, num_ens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYIIMV3T7iC"
   },
   "outputs": [],
   "source": [
    "from uncertainty_estimation import init_dataset, get_uncertainty_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPmcyYoUmie"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bX = next(iter(test_loader))\n",
    "img = bX[0][2].to(device) # Chose the one you want\n",
    "\n",
    "\n",
    "def softmax(X): \n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "pred, epistemic, aleatoric = get_uncertainty_per_image(net, img)\n",
    "cls = pred.argmax()\n",
    "proba = softmax(pred)\n",
    "print(f\"{cls} : ({proba[cls]:.2})\")\n",
    "plt.plot(proba);\n",
    "plt.plot(epistemic);\n",
    "plt.plot(aleatoric);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A4OdgVIV-S_"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img.cpu().numpy().transpose(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98bB8mAede-m"
   },
   "source": [
    "If you want to train this model yourself, you may run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVft8BghC7aX"
   },
   "outputs": [],
   "source": [
    "!python main_bayesian.py --net_type 3conv3fc --dataset CIFAR10"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BayesianDeepWine",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

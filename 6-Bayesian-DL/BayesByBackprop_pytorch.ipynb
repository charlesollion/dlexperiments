{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5fU0bcRFM7B"
   },
   "source": [
    "# Introduction to Bayesian DL\n",
    "\n",
    "Click to run on colab (if you're not already there): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/BayesByBackprop_pytorch.ipynb) \n",
    "\n",
    "This session aims at understanding and implementing basic Bayesian Deep Learning models, as described in [Bayes by Backprop](https://arxiv.org/abs/1505.05424).\n",
    "\n",
    "![](https://github.com/charlesollion/dlexperiments/raw/master/6-Bayesian-DL/BDLworkflow.png)\n",
    "\n",
    "**What this is about:**\n",
    "\n",
    "- fitting the Pytorch framework\n",
    "- understanding basic BBB building blocks\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "The notebook itself is inspired from [Khalid Salama's Keras tutorial on Bayesian Deep Learning](https://keras.io/examples/keras_recipes/bayesian_neural_networks/), and takes several graphs from the excellent paper [Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users\n",
    "](https://arxiv.org/abs/2007.06823). If you're interested in the Keras/Tensorflow version, please consider this instead:\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/BayesianDeepWine.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why BayesByBackprop\n",
    "\n",
    "Our goal is to use existing frameworks such as Pytorch, and stick to a standard way of training Neural Networks:\n",
    "\n",
    "- We will use Stochastic Gradient Descent (SGD) or a variant\n",
    "- We will then train on random mini-batches of data (data may be large)\n",
    "- We want to take advantage of automatic differentiation and not compute gradients ourselves: everything must be **differentiable**\n",
    "- We want to benefit from parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjW7a270FM7G"
   },
   "source": [
    "### Installation (outside of Colab)\n",
    "\n",
    "You just need pytorch and the built-in `torch.distributions` will be used:\n",
    "\n",
    "```python\n",
    "pip install pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJcfG-w_FM7H"
   },
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab, you may download the dataset running this cell\n",
    "!wget https://github.com/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/X_data.npy?raw=true -O X_data.npy\n",
    "!wget https://github.com/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/y_data.npy?raw=true -O y_data.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJcfG-w_FM7H"
   },
   "source": [
    "We use the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality)\n",
    "dataset.\n",
    "We use the red wine subset, which contains 4,898 examples.\n",
    "The dataset has 11 numerical physicochemical features of the wine, and the task is to predict the wine quality, which is a score between 0 and 10.\n",
    "\n",
    "While the experts gave integer scores, we will first consider them as continuous values between 0 and 10 and treat this as a regression task, for simplicity and easy interpretation of confidence intervals. To make the data more continous, we add a small observation noise to `y`.\n",
    "\n",
    "We could consider instead consider a classification task, or many different things here, but that's not the point of this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYDrwmZTFM7K"
   },
   "source": [
    "### Create training and evaluation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06fXwfRCFM7Q"
   },
   "source": [
    "Let's split the wine dataset into training and test sets, with 85% and 15% of\n",
    "the examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# X has 11 continuous inputs\n",
    "# y is treated as a continous rating between 0 and 9\n",
    "FEATURE_NAMES = [\n",
    "    \"fixed acidity\", \"volatile acidity\", \"citric acid\",\n",
    "    \"residual sugar\", \"chlorides\", \"free sulfur dioxide\",\n",
    "    \"total sulfur dioxide\", \"density\", \"pH\",\n",
    "    \"sulphates\", \"alcohol\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "all_X_data = np.load(\"./X_data.npy\")\n",
    "all_y_data = np.load(\"./y_data.npy\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_X_data, all_y_data, test_size=0.15, random_state=42)\n",
    "\n",
    "# add a bit of noise to y_train\n",
    "y_train = y_train + np.random.normal(0,0.2, size=y_train.shape)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to torch tensor\n",
    "tensor_X_train = torch.Tensor(X_train) \n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "tensor_X_test = torch.Tensor(X_test) \n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "# build dataset and dataloader torch objects\n",
    "dataset_train = TensorDataset(tensor_X_train, tensor_y_train)\n",
    "dataset_test = TensorDataset(tensor_X_test, tensor_y_test)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUEGoFbJFM7P"
   },
   "source": [
    "## Baseline: Standard neural network\n",
    "\n",
    "We create a standard deterministic neural network model as a baseline.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/charlesollion/dlexperiments/master/6-Bayesian-DL/stochasticNN.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyqun6Ch2IaQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaselineTorchModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(BaselineTorchModel, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.act = torch.relu\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = self.hidden_layer(inputs)\n",
    "        h = self.act(h)\n",
    "        output = self.out_layer(h)\n",
    "        \n",
    "        # we add a dummy output, a placeholder for future experiments\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-yBAJje3LEd"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, loss, num_epochs):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.003)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        for x, y in dataloader_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss_value = loss(model(x), y)\n",
    "            loss_value.backward()\n",
    "            losses.append(loss_value.detach().cpu().item())\n",
    "            optimizer.step()\n",
    "    return losses\n",
    "\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    for x,y in dataloader_test:\n",
    "        y_hat, _ = model(x)\n",
    "        errors.append(((torch.squeeze(y_hat) - torch.squeeze(y))**2).detach().cpu().numpy())\n",
    "  \n",
    "    rmse = np.sqrt(np.mean(np.concatenate(errors, axis=None)))\n",
    "    return round(rmse, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smdJBd6q287-"
   },
   "outputs": [],
   "source": [
    "baseline_torch_model = BaselineTorchModel(11, 32).to(device)\n",
    "baseline_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.numel() for p in baseline_torch_model.parameters()]\n",
    "# hidden layer W, hidden layer b, output layer W, output layer b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline evaluations\n",
    "\n",
    "Let's see the untrained model RMSE, and then a very stupid constant model (just predicting the mean of `y_train`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = eval_model(baseline_torch_model)\n",
    "print(f\"untrained RMSE: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant():\n",
    "    def eval(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Always return 5.81...\n",
    "        return torch.ones((x.shape[0], 1)) * np.mean(y_train), None\n",
    "\n",
    "rmse = eval_model(Constant())\n",
    "print(f\"constant model RMSE: {round(rmse, 3):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smdJBd6q287-"
   },
   "outputs": [],
   "source": [
    "def simple_mse_loss(model_outputs, y_true):\n",
    "    y_hat, _ = model_outputs\n",
    "    y_hat = torch.squeeze(y_hat)\n",
    "    y_true = torch.squeeze(y_true)\n",
    "    return torch.nn.MSELoss()(y_hat, y_true) \n",
    "\n",
    "losses = train_model(baseline_torch_model, simple_mse_loss, 50)\n",
    "\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = eval_model(baseline_torch_model)\n",
    "print(f\"untrained RMSE: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KlQB2lY8UdN"
   },
   "outputs": [],
   "source": [
    "samples = 10\n",
    "examples_torch, targets_torch = next(iter(dataloader_test))\n",
    "predicted, _ = baseline_torch_model(examples_torch[:samples])\n",
    "predicted = predicted.detach().cpu().numpy()\n",
    "for idx in range(samples):\n",
    "    print(f\"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets_torch[idx].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWS8r4qKmf5z"
   },
   "source": [
    "## A simple Bayesian Neural Network\n",
    "\n",
    "Our objective is to build a single layer Bayesian Neural Network using Tensorflow or Pytorch. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/charlesollion/dlexperiments/master/6-Bayesian-DL/stochasticNN.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "We define a unit Gaussian prior, and a diagonal covariance multivariate Gaussian posterior.\n",
    "\n",
    "From the [Bayes by Backprop](https://arxiv.org/abs/1505.05424) paper, we have the following algorithm:\n",
    "\n",
    "1. Sample $\\epsilon ∼ N(0, I)$.\n",
    "2. Let $w = µ + log(1 + exp(ρ)) ◦ \\epsilon$.\n",
    "3. Let $θ = (µ, ρ)$.\n",
    "4. Let $f(w, θ) = log q(w|θ) − log P(w)P(D|w)$.\n",
    "5. Compute gradients with respect to everything\n",
    "\n",
    "Note that the $∂f(w,θ) ∂w$ term of the gradients are exactly the gradients found by the usual backpropagation algorithm on a neural network. Thus, remarkably, to learn both the mean and the standard deviation we must simply calculate the usual gradients found by backpropagation.\n",
    "\n",
    "When working on $M$ mini-batches $D_i$ of data, the authors propose the (equivalent) following cost function to differentiate:\n",
    "\n",
    "$$f(w, θ, D_i) = − log P(D_i|w) + \\frac{1}{M} KL[q(w|θ) || P(w)] $$\n",
    "\n",
    "This is convenient as the cost function is seperated in two terms, one which corresponds to the standard mse loss as before, and the other a regularization term: the KL divergence between the posterior q and the prior. Note that there is a weight ($\\frac{1}{M}$) which can be chosen with different schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDaE3yXxD27U"
   },
   "source": [
    "### Simple Bayesian By Backprop Layer\n",
    "\n",
    "In the following, we start by implementing a linear layer consisting stochastic weights. The output layer is kept as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TY8J6QW8vHCG"
   },
   "outputs": [],
   "source": [
    "class BnnTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation=None):\n",
    "        super(BnnTorch, self).__init__()\n",
    "        n = input_dim * hidden_dim\n",
    "        self.mu = nn.Parameter(torch.zeros((n), dtype=torch.float32))\n",
    "        self.rho  = nn.Parameter(torch.log(torch.expm1(torch.ones((n), dtype=torch.float32))))\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.act = activation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prior = torch.distributions.Normal(loc=torch.zeros((n), device=device, dtype=torch.float32),\n",
    "                                                scale=torch.ones((n), device=device, dtype=torch.float32))\n",
    "        #self.posterior = torch.distributions.Normal(loc=self.mu, \n",
    "        #                                            scale=torch.log(1.+torch.exp(self.rho)))\n",
    "        self.kl_func = torch.distributions.kl.kl_divergence\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(input_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.batch_norm(inputs)\n",
    "        q = torch.distributions.Normal(loc=self.mu, \n",
    "                                       scale=torch.log(1.+torch.exp(self.rho)))\n",
    "        \n",
    "        kl = torch.sum(self.kl_func(q, self.prior))\n",
    "        # we use q.rsample() which uses the reparametrization trick instead of \n",
    "        # q.sample() which breaks the auto-differentation path\n",
    "        w = q.rsample() \n",
    "        w = w.reshape((-1, self.hidden_dim))\n",
    "        h = inputs @ w\n",
    "        if self.act is not None:\n",
    "            h = self.act(h)\n",
    "        output = self.out_layer(h)\n",
    "        return output, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9CErey5vW84",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bnn_torch = BnnTorch(11, 32, torch.nn.functional.relu).to(device)\n",
    "[p.numel() for p in bnn_torch.parameters()]\n",
    "#mu, rho, W_output, b_output, batch_norm mu, batch_norm sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9CErey5vW84"
   },
   "outputs": [],
   "source": [
    "kl_weight = 2. / batch_size\n",
    "\n",
    "def mse_kl_loss(model_outputs, y_true):\n",
    "    y_hat, kl = model_outputs\n",
    "    y_hat = torch.squeeze(y_hat)\n",
    "    y_true = torch.squeeze(y_true)\n",
    "    mse = torch.nn.MSELoss()(y_hat, y_true)\n",
    "    return mse + kl * kl_weight\n",
    "\n",
    "losses = train_model(bnn_torch, mse_kl_loss, 200)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuxe0T3fzVTJ"
   },
   "outputs": [],
   "source": [
    "def compute_predictions(model, iterations=20):\n",
    "    model.eval()\n",
    "    # We sample different weights for each example\n",
    "    # This is slow! We don't batch anything, and do \n",
    "    single_item_dataloader_test = DataLoader(dataset_test, batch_size=1)\n",
    "    predicted = np.zeros((y_test.shape[0], iterations))\n",
    "    \n",
    "    for j, (x, y) in tqdm(enumerate(single_item_dataloader_test)):\n",
    "        for i in range(iterations):\n",
    "            y_hat, _ = model(x)\n",
    "            predicted[j,i] = y_hat.detach().cpu()[0].item()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(predictions, targets, samples=10):\n",
    "    prediction_mean = np.mean(predictions, axis=1).tolist()\n",
    "    prediction_min = np.min(predictions, axis=1).tolist()\n",
    "    prediction_max = np.max(predictions, axis=1).tolist()\n",
    "    prediction_range = (np.max(predictions, axis=1) - np.min(predictions, axis=1)).tolist()\n",
    "\n",
    "    for idx in range(samples):\n",
    "        print(\n",
    "            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n",
    "            f\"min: {round(prediction_min[idx], 2)}, \"\n",
    "            f\"max: {round(prediction_max[idx], 2)}, \"\n",
    "            f\"range: {round(prediction_range[idx], 2)} - \"\n",
    "            f\"Actual: {targets[idx].item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLqsO83iy2Dl"
   },
   "outputs": [],
   "source": [
    "predictions = compute_predictions(bnn_torch)\n",
    "display_predictions(predictions, targets_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "mean_prediction = np.mean(predictions,axis=-1)\n",
    "for y_hat, y in zip(mean_prediction, y_test):\n",
    "    errors.append((y_hat - y)**2)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.concatenate(errors, axis=None)))\n",
    "print(f\"mean prediction RMSE: {round(rmse, 3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=np.random.choice(range(predictions.shape[0]), size=10, replace=False)\n",
    "plt.boxplot(predictions[idx].T)\n",
    "plt.plot(range(1,11), y_test[idx], 'r.', alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Bayesian Network\n",
    "\n",
    "We will now implement the full baysian neural network, exactly as stated in the paper. This corresponds to adding stochastic biases and output weights.\n",
    "\n",
    "Keep in mind that the posterior over each weight/bias is still an independant Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullBnnTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation=None):\n",
    "        super(FullBnnTorch, self).__init__()\n",
    "        # All parameters W, b + W_output + b_output\n",
    "        n = input_dim * hidden_dim + hidden_dim + hidden_dim + 1\n",
    "        self.mu = nn.Parameter(torch.zeros((n), dtype=torch.float32))\n",
    "        self.rho  = nn.Parameter(torch.log(torch.expm1(torch.ones((n), dtype=torch.float32))))\n",
    "        \n",
    "        self.act = activation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.prior = torch.distributions.Normal(loc=torch.zeros((n), device=device, dtype=torch.float32),\n",
    "                                                scale=torch.ones((n), device=device, dtype=torch.float32))\n",
    "        #self.posterior = torch.distributions.Normal(loc=self.mu, \n",
    "        #                                            scale=torch.log(1.+torch.exp(self.rho)))\n",
    "        self.kl_func = torch.distributions.kl.kl_divergence\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(input_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.batch_norm(inputs)\n",
    "        q = torch.distributions.Normal(loc=self.mu, \n",
    "                                       scale=torch.log(1.+torch.exp(self.rho)))\n",
    "        \n",
    "        kl = torch.sum(self.kl_func(q, self.prior))\n",
    "        # we use q.rsample() which uses the reparametrization trick instead of \n",
    "        # q.sample() which breaks the auto-differentation path\n",
    "        all_w = q.rsample() \n",
    "        \n",
    "        # split all_w into the different weight and biases matrices\n",
    "        W_hidden = all_w[0:self.input_dim * self.hidden_dim].reshape((self.input_dim, self.hidden_dim))\n",
    "        cur = self.input_dim * self.hidden_dim\n",
    "        b_hidden = all_w[cur:cur + self.hidden_dim]\n",
    "        cur = cur + self.hidden_dim\n",
    "        W_output = all_w[cur:cur + self.hidden_dim].reshape((self.hidden_dim, 1))\n",
    "        b_output = all_w[-2:-1]\n",
    "        h = inputs @ W_hidden + b_hidden\n",
    "        if self.act is not None:\n",
    "            h = self.act(h)\n",
    "        output = h @ W_output + b_output\n",
    "        return output, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9CErey5vW84",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_bnn_torch = FullBnnTorch(11, 32, torch.nn.functional.relu).to(device)\n",
    "[p.numel() for p in full_bnn_torch.parameters()]\n",
    "# 417 = 11*32 + 32 + 32*1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9CErey5vW84"
   },
   "outputs": [],
   "source": [
    "kl_weight = 1. / batch_size\n",
    "\n",
    "def mse_kl_loss(model_outputs, y_true):\n",
    "    y_hat, kl = model_outputs\n",
    "    y_hat = torch.squeeze(y_hat)\n",
    "    y_true = torch.squeeze(y_true)\n",
    "    mse = torch.nn.MSELoss()(y_hat, y_true)\n",
    "    return mse + kl * kl_weight\n",
    "\n",
    "losses = train_model(full_bnn_torch, mse_kl_loss, 200)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLqsO83iy2Dl"
   },
   "outputs": [],
   "source": [
    "predictions = compute_predictions_torch(bnn_torch)\n",
    "display_predictions(predictions, targets_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "mean_prediction = np.mean(predictions,axis=-1)\n",
    "for y_hat, y in zip(mean_prediction, y_test):\n",
    "    errors.append((y_hat - y)**2)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.concatenate(errors, axis=None)))\n",
    "print(f\"mean prediction RMSE: {round(rmse, 3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx=np.random.choice(range(predictions.shape[0]), size=10, replace=False)\n",
    "plt.boxplot(predictions[idx].T)\n",
    "plt.plot(range(1,11), targets_torch[idx].cpu(), 'r.', alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53tmcKY_FM7X"
   },
   "source": [
    "## Comparison with the parametrizing a distribution\n",
    "\n",
    "So far, we have distributions over weights, but single point estimates for each input (once we sample these weights).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/charlesollion/dlexperiments/master/6-Bayesian-DL/stochasticNN.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "We can also model an output distribution, rather than a single point estimate. We will change the last layer to become a parametrization of a Gaussian as the output. This should be related to *aleatoric uncertainty*, irreducible noise in the data, capturing the stochastic nature of the process generating the data.\n",
    "\n",
    "We then use the negative loglikelihood of this distribution as our loss function, instead of the MSE. We are not using any stochastic weights in that case (it would still be possible!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DistributionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DistributionModel, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        # We output two values, which will be interpreted as mu and log sigma of a distribution\n",
    "        self.out_layer = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "        self.act = torch.relu\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = self.hidden_layer(inputs)\n",
    "        h = self.act(h)\n",
    "        output = self.out_layer(h)\n",
    "        mu, logscale = torch.split(output, [1,1], dim=-1)\n",
    "        output = torch.distributions.Normal(loc=mu,\n",
    "                                            scale=torch.exp(logscale))\n",
    "        \n",
    "        # we return a distribution instead of a single point estimate\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_model = DistributionModel(11, 32).to(device)\n",
    "[p.numel() for p in distrib_model.parameters()]\n",
    "# 11*32 + 32 + 32*2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(model_outputs, y_true):\n",
    "    return - torch.sum(model_outputs.log_prob(y_true))\n",
    "\n",
    "losses = train_model(distrib_model, nll_loss, 200)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_model.eval()\n",
    "errors = []\n",
    "intervals = []\n",
    "for x,y in dataloader_test:\n",
    "    d = distrib_model(x)\n",
    "    y_hat = d.mean\n",
    "    intervals.append(d.scale.detach().cpu().numpy())\n",
    "    errors.append(((torch.squeeze(y_hat) - torch.squeeze(y))**2).detach().cpu().numpy())\n",
    "    \n",
    "\n",
    "rmse = np.sqrt(np.mean(np.concatenate(errors, axis=None)))\n",
    "inter = np.mean(np.concatenate(intervals, axis=None)) * 1.96 * 2\n",
    "print(f\"mean prediction RMSE: {round(rmse, 3):.3f}\")\n",
    "print(f\"average 95% confidence interval size: {round(inter, 3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_d = distrib_model(examples_torch[:samples])\n",
    "predicted_means = predicted_d.mean.detach().cpu().numpy()\n",
    "predicted_sigmas = predicted_d.scale.detach().cpu().numpy()\n",
    "\n",
    "upper = (predicted_means + (1.96 * predicted_sigmas)).tolist()\n",
    "lower = (predicted_means - (1.96 * predicted_sigmas)).tolist()\n",
    "\n",
    "for idx in range(samples):\n",
    "    print(\n",
    "        f\"Prediction mean: {round(predicted_means[idx][0], 2):.2f}, \"\n",
    "        f\"stddev: {round(predicted_sigmas[idx][0], 2):.2f}, \"\n",
    "        f\"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]\"\n",
    "        f\" - Actual: {y_test[idx].item()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.choice(range(samples), size=10, replace=False)\n",
    "predictions = predicted_d.sample((20,))[:,:,0]\n",
    "plt.boxplot(predictions[idx].T)\n",
    "plt.plot(range(1,11), y_test[idx], 'r.', alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pl6D20rbCJSe"
   },
   "source": [
    "## 6) More complex examples: Convolutional models\n",
    "\n",
    "This bonus section showcases a larger bayesian neural network with convolutional layers. \n",
    "\n",
    "##### In Physics\n",
    "\n",
    "A good example of a practical use of Deep Bayesian Learning in Physics Simulation is available here: [Thuerey Group Physics Deep Learning](https://www.physicsbaseddeeplearning.org/bayesian-code.html)\n",
    "\n",
    "The goal is to predict the air flow around an airfoil, and BDL enables to produce several distinct plausible outputs:\n",
    "<img src=\"https://www.physicsbaseddeeplearning.org/_images/bayesian-code_27_0.png\" width=\"600\">\n",
    "\n",
    "##### A standard bayesian convnet \n",
    "A simpler convolutional example is extracted from a [github repository](https://github.com/kumar-shridhar/PyTorch-BayesianCNN). It is trained on CIFAR10.\n",
    "\n",
    "All weights, including the convolutionnal kernels are sampled from a weight distribution:\n",
    "![](https://github.com/kumar-shridhar/PyTorch-BayesianCNN/raw/master/experiments/figures/CNNwithdist_git.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMY-6319CMfw"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kumar-shridhar/PyTorch-BayesianCNN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSosnOyDCqj7"
   },
   "outputs": [],
   "source": [
    "%cd PyTorch-BayesianCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITrWrC--LyxO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import data\n",
    "import utils\n",
    "import metrics\n",
    "import config_bayesian as cfg\n",
    "from models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
    "\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LVK8mphL64t"
   },
   "outputs": [],
   "source": [
    "trainset, testset, inputs, outputs = data.getDataset(\"CIFAR10\")\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "        trainset, testset, cfg.valid_size, 32, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a pretrained model (could be more trained!). Warning, it might only work on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/charlesollion/dlexperiments/blob/master/6-Bayesian-DL/model_3conv3fc_lrt_softplus.pt?raw=true -O model_3conv3fc_lrt_softplus.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7R9SMK0K8t4"
   },
   "outputs": [],
   "source": [
    "weights_path = \"model_3conv3fc_lrt_softplus.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxyvHauVNHw9"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(weights_path, map_location=device)\n",
    "net = BBB3Conv3FC(outputs, inputs, cfg.priors, cfg.layer_type, cfg.activation_type).to(device)\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6FeZ63eO7i8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def validate_model(net, validloader, num_ens=1):\n",
    "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
    "    net.eval()\n",
    "    accs = []\n",
    "\n",
    "    for i, (inputs, labels) in tqdm(enumerate(validloader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
    "        for j in range(num_ens):\n",
    "            net_out, _ = net(inputs)\n",
    "            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
    "\n",
    "        log_outputs = utils.logmeanexp(outputs, dim=2)\n",
    "        accs.append(metrics.acc(log_outputs, labels))\n",
    "\n",
    "    return np.mean(accs)\n",
    "\n",
    "validate_model(net, test_loader, num_ens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYIIMV3T7iC"
   },
   "outputs": [],
   "source": [
    "from uncertainty_estimation import init_dataset, get_uncertainty_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPmcyYoUmie"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bX = next(iter(test_loader))\n",
    "img = bX[0][2].to(device) # Chose the one you want\n",
    "\n",
    "\n",
    "def softmax(X): \n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "pred, epistemic, aleatoric = get_uncertainty_per_image(net, img)\n",
    "cls = pred.argmax()\n",
    "proba = softmax(pred)\n",
    "print(f\"{cls} : ({proba[cls]:.2})\")\n",
    "plt.plot(proba);\n",
    "plt.plot(epistemic);\n",
    "plt.plot(aleatoric);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A4OdgVIV-S_"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img.cpu().numpy().transpose(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98bB8mAede-m"
   },
   "source": [
    "If you want to train this model yourself, you may run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVft8BghC7aX"
   },
   "outputs": [],
   "source": [
    "!python main_bayesian.py --net_type 3conv3fc --dataset CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BayesianDeepWine",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

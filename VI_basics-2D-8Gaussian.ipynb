{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference \n",
    "#### Basics: Evidence Lower Bound (ELBO) & Coordinate Ascent Variational Inference (CAVI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Required packages\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Mixture of Gaussian distributions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following Bayesian mixture of 8 2-dimensionnal Gaussians. \n",
    "\n",
    "There are $K = 8$ mixture components, each component $k$ of the mixture is a unit Gaussian distribution with mean $\\mathbf{\\mu_k}$. The weight of each mixture component $k$ written $\\omega_k$. The observations $(X_i)_{1\\leqslant i\\leqslant n}$ are assumed to be i.i.d. with probability density:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^K \\omega_k \\varphi_{\\mathbf{\\mu_k,I}}(x)\\,,\n",
    "$$\n",
    "\n",
    "where $\\varphi_{\\mu_k,\\sigma^2}$ is the Gaussian probability function with mean $\\mathbf{\\mu_k}$. The likelihood is then given by:\n",
    "\n",
    "$$\n",
    "p(x_1,\\cdots,x_n) = \\int p(x_1,\\cdots,x_n|\\mu) p(\\mu) \\mathrm{d} \\mu = \\int \\prod_{i=1}^n p(x_i|\\mu) p(\\mu) \\mathrm{d} \\mu = \\int \\prod_{i=1}^n \\left(\\sum_{k=1}^K \\omega_k \\varphi_{\\mu_k,1}(x_i)\\right) p(\\mu) \\mathrm{d} \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application with $K= 8$, $\\sigma^2 = 0.2$, $\\omega_k = 1/K$ for all $1\\leqslant k \\leqslant K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eightgaussian(n_points):\n",
    "    \"\"\"\n",
    "     Returns the eight gaussian dataset.\n",
    "    \"\"\"\n",
    "    n = np.random.randint(0,8, n_points)\n",
    "    noisex = np.random.normal(size=(n_points)) * 0.2\n",
    "    noisey = np.random.normal(size=(n_points)) * 0.2\n",
    "    x_centers,y_centers = [np.cos(n* np.pi/4.0) * 5 + noisex, np.sin(n* np.pi/4.0) * 5 + noisey]\n",
    "    return np.vstack((x_centers,y_centers)).T\n",
    "            \n",
    "X = eightgaussian(1000)\n",
    "X.shape\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0], X[:,1], s=1);\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to approximate the posterior distribution $p(\\mu,c|x)$ where $c = (c_1,\\cdots,c_n)$ are the mixture components of the observations.  The mean-field variational family is described as follows:\n",
    "\n",
    "$$\n",
    "q(\\mu,c) = \\prod_{k=1}^K \\varphi_{m_k,s_k}(\\mu_k)\\prod_{i=1}^n \\mathrm{Cat}_{\\phi_i}(c_i)\\,, \n",
    "$$\n",
    "\n",
    "which means that:\n",
    "\n",
    "- $\\mu$ and $c$ are independent.\n",
    "- $(\\mu_{k})_{1\\leqslant k \\leqslant K}$ are independent with Gaussian distribution with means $(m_{k})_{1\\leqslant k \\leqslant K}$ and variances $(s_{k})_{1\\leqslant k \\leqslant K}$.\n",
    "- $(c_{i})_{1\\leqslant i \\leqslant n}$ are independent with multinomial distribution with parameters $(\\phi_i)_{1\\leqslant i \\leqslant n}$: $q(c_i=k) = \\phi_i(k)$ for $1\\leqslant k \\leqslant K$. \n",
    "\n",
    "Write $\\mathcal{D}$ the family of such distributions when $(m_{k})_{1\\leqslant k \\leqslant K}\\in \\mathbb{R}^K$, variances $(s_{k})_{1\\leqslant k \\leqslant K}\\in (\\mathbb{R}_+^*)^K$ and $(\\phi_i)_{1\\leqslant i \\leqslant n}\\in \\mathcal{S}_K^n$ where $\\mathcal{S}_K$ is the $K$-dimensional probability simplex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is now to find the\n",
    "\"best candidate\" in $\\mathcal{D}$ to approximate $p(\\mu,c|x)$, i.e. the one ``which minimizes the following KL divergence``:\n",
    "\n",
    "$$\n",
    "q^* = \\mathrm{Argmin}_{q\\in\\mathcal{D}} \\mathrm{KL}\\left(q(\\mu,c)\\|p(\\mu,c|x)\\right)\\,.\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\\begin{align*}\n",
    "\\mathrm{KL}\\left(q(\\mu,c)\\|p(\\mu,c|x)\\right) &= \\mathbb{E}_q[\\log q(\\mu,c)] - \\mathbb{E}_q[\\log p(\\mu,c|x)]\\,,\\\\\n",
    " &= \\mathbb{E}_q[\\log q(\\mu,c)] - \\mathbb{E}_q[\\log p(\\mu,c,x)]+\\log p(x)\\,,\\\\\n",
    "&= -\\mathrm{ELBO}(q)+\\log p(x)\\,,\n",
    "\\end{align*}\n",
    "\n",
    "where the ``Evidence Lower Bound`` (ELBO) is\n",
    "\n",
    "$$\n",
    "\\mathrm{ELBO}(q) = -\\mathbb{E}_q[\\log q(\\mu,c)] + \\mathbb{E}_q[\\log (\\mu,c,x)] \\,.\n",
    "$$\n",
    "\n",
    "Therefore, ``minimizing the KL divergence`` boils down to maximizing the ELBO, where $\\log p(x)\\geqslant \\mathrm{ELBO}(q)$.\n",
    "\n",
    "The complexity of the family $\\mathcal{D}$ determines the complexity of the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic CAVI algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detail here why the algorithm computes iteratively for all $1\\leqslant k \\leqslant K$,\n",
    "\n",
    "$$\n",
    "q(\\mu_k) \\propto \\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_k(\\mu_k|x)]\\right)\n",
    "$$\n",
    "\n",
    "and for all $1\\leqslant i \\leqslant n$,\n",
    "\n",
    "$$\n",
    "q(c_i) \\propto \\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)]\\right)\\,,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\tilde p_i(c_i|x)$ is the conditional distribution of $c_i$ given the observations and all the other parameters and $\\tilde p_k(\\mu_k|x)$ is the conditional distribution of $\\mu_k$ given the observations and all the other parameters.\n",
    "\n",
    "- $\\mathbb{E}_{\\tilde q_z}$ is the expectation under the variational law of all parameters except $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to the mixture of Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\tilde p_i(c_i|x)$ be the conditional distribution of $c_i$ given the observations and the other parameters.\n",
    "\n",
    "$$\n",
    "\\tilde p_i(c_i|x) \\propto p(c_i)p(x_i|c_i,\\mu) \\propto p(c_i)\\prod_{k=1}^K \\left(\\varphi_{\\mu_k,1}(x_i)\\right)^{1_{c_i=k}}\\,. \n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)] = \\log p(c_i) + \\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[\\log \\varphi_{\\mu_k,1}(x_i)]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)]\\right) &\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[-(x_i-\\mu_k)^2/2]\\right)\\,\\\\\n",
    "&\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[-(x_i-\\mu_k)^2/2]\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The update is then written:\n",
    "\n",
    "$$\n",
    "\\varphi_i(k) \\propto p(c_i=k) \\mathrm{exp}\\left(m_k x_i - \\frac{m^2_k + s_k}{2}\\right)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update of $(\\phi_i)_{1\\leqslant i \\leqslant n}$ using CAVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_update_phi(X,m,s2):\n",
    "    \n",
    "    first_term_mean = np.dot(X, m.T)\n",
    "    second_term_mean = -(m**2 + s2).sum(axis=-1)/2\n",
    "    \n",
    "    phi = np.exp(first_term_mean + second_term_mean)\n",
    "    phi = phi / phi.sum(1)[:, np.newaxis]\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\tilde p_k(\\mu_k|x)$ be the conditional distribution of $\\mu_k$ given the observations and the other parameters.\n",
    "\n",
    "$$\n",
    "\\tilde p_k(\\mu_k|x) \\propto p(\\mu_k)\\prod_{i=1}^np(x_i|c_i,\\mu) \\propto p(\\mu_k)\\prod_{i=1}^n p(x_i|\\mu,c_i)\\,. \n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_k(\\mu_k|x)] = \\log p(\\mu_k) + \\sum_{i=1}^n \\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log p(x_i|\\mu,c_i)]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_i(c_i|x)]\\right) &\\propto p(\\mu_k) \\mathrm{exp}\\left(\\sum_{i=1}^n\\sum_{k=1}^K  \\mathbb{E}_{\\tilde q_{\\mu_k}}[1_{c_i=k}\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto p(\\mu_k) \\mathrm{exp}\\left(\\sum_{i=1}^n \\phi_i(k) \\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto \\mathrm{exp}\\left(-\\frac{\\mu_k^2}{2\\sigma^2}-\\frac{1}{2}\\sum_{i=1}^n \\phi_i(k)(x_i-\\mu_k)^2\\right)\\,,\\\\\n",
    "&\\propto \\mathrm{exp}\\left(-\\frac{\\mu_k^2}{2\\sigma^2}+\\sum_{i=1}^n \\phi_i(k)x_i\\mu_k - \\frac{1}{2}\\sum_{i=1}^n \\phi_i(k)\\mu^2_k\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The update is then written:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{i=1}^n \\phi_i(k)x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_i(k)}\\quad\\mathrm{and}\\quad s_k = \\frac{1}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_i(k)}\\,. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update of $(m_{k})_{1\\leqslant k \\leqslant K}$ and $(s_{k})_{1\\leqslant k \\leqslant K}$ using CAVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_update_mu_s2(X,m,phi,s2,sigma2,K):\n",
    "    \n",
    "    phi_sum = phi.sum(0)[:,np.newaxis]\n",
    "    m  = np.dot(phi.T,X) * (1/sigma2 + phi_sum)**(-1)\n",
    "    s2 = (1/sigma2 + phi_sum)**(-1)\n",
    "    print(f\"m {m.shape}, s2 {s2.shape}\")\n",
    "    return m, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(X,phi,m,s2,sigma2):\n",
    "        \n",
    "        first_term  = (np.log(s2) - m/sigma2).sum()\n",
    "        second_term = (-0.5*np.dot(X**2, (s2+m**2).T) + np.dot(X, m.T) - np.log(phi))*phi\n",
    "\n",
    "        return first_term + second_term.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_mixture_Gaussian(X,m, s2, phi, sigma2, max_iter = 500, epsilon = 1e-8):\n",
    "        \n",
    "        elbos  = [elbo(X,phi,m,s2,sigma2)]\n",
    "        m_est  = [m]\n",
    "        s2_est = [s2]\n",
    "        \n",
    "        for it in range(1, max_iter+1):\n",
    "            \n",
    "            phi   = CAVI_update_phi(X,m,s2)\n",
    "            m, s2 = CAVI_update_mu_s2(X,m,phi,s2,sigma2,K)\n",
    "            \n",
    "            m_est.append(m)\n",
    "            s2_est.append(s2)\n",
    "            \n",
    "            elbos.append(elbo(X,phi,m,s2,sigma2))\n",
    "\n",
    "            if np.abs(elbos[-2] - elbos[-1]) <= epsilon:\n",
    "                break\n",
    "        \n",
    "        return elbos, m_est, s2_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "K = 9\n",
    "\n",
    "phi_init = np.random.dirichlet([np.random.random()*np.random.randint(1, 10)]*K, n_samples)\n",
    "m_init = np.random.normal(0, 1, (K,2))\n",
    "s2_init = np.random.random((K,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "elbos, m_est, s2_est = CAVI_mixture_Gaussian(X, m_init, s2_init, phi_init, np.array([3., 3.]), 500, 0.001)\n",
    "plt.plot(np.array(elbos),label = 'ELBO')\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0], X[:,1], s=1);\n",
    "plt.scatter(m_est[-1][:,0],m_est[-1][:,1], s=50, c=\"r\")\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est[-1], s2_est[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

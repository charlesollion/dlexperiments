{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle KPP\n",
    "\n",
    "Tools: jax (to install `pip install jax jaxlib flax`)\n",
    "If you are unfamiliar with Jax random generation, check [this](https://jax.readthedocs.io/en/latest/jax.random.html)\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{eq:KPP_homog}\n",
    "  \\partial_t u(t,x) = D \\Delta u + r u (1 - u), \\ t>0, \\ x\\in \\Omega \\subset \\mathbb{R}^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "avec la condition initiale $u(0,\\cdot)=u_0(\\cdot)$ une gausienne dans $\\Omega$ très \"peakée\", et la condition au bord $u(t,\\cdot )=0$ sur $\\partial\\Omega$ pour tout $t>0$. On pourra prendre $\\Omega=(0,1)\\times(0,1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " FrozenDict({\n",
      "    params: {\n",
      "        layers_0: {\n",
      "            bias: (20,),\n",
      "            kernel: (3, 20),\n",
      "        },\n",
      "        layers_1: {\n",
      "            bias: (40,),\n",
      "            kernel: (20, 40),\n",
      "        },\n",
      "        layers_2: {\n",
      "            bias: (80,),\n",
      "            kernel: (40, 80),\n",
      "        },\n",
      "        layers_3: {\n",
      "            bias: (80,),\n",
      "            kernel: (80, 80),\n",
      "        },\n",
      "        layers_4: {\n",
      "            bias: (80,),\n",
      "            kernel: (80, 80),\n",
      "        },\n",
      "        layers_5: {\n",
      "            bias: (40,),\n",
      "            kernel: (80, 40),\n",
      "        },\n",
      "        layers_6: {\n",
      "            bias: (20,),\n",
      "            kernel: (40, 20),\n",
      "        },\n",
      "        layers_7: {\n",
      "            bias: (1,),\n",
      "            kernel: (20, 1),\n",
      "        },\n",
      "    },\n",
      "})\n",
      "\n",
      "u(x, t): -0.026\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "from models.nets import MLP\n",
    "from functools import partial\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "# A test point\n",
    "x_test = np.ones(2) * 0.25\n",
    "t_test = np.ones(1) * 0.25\n",
    "\n",
    "model = MLP(features=[20,40,80,80,80,40,20, 1])\n",
    "init_params = model.init(subkey, t_test, x_test)\n",
    "\n",
    "@jit\n",
    "def u(t, x, params_):\n",
    "    return model.apply(params_, t, x)[0]\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_map(np.shape, init_params))\n",
    "print(f'\\nu(x, t): {u(t_test, x_test, init_params):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1.0\n",
    "r = 1.0\n",
    "\n",
    "def hessian(f, index_derivation=0):\n",
    "    return jacfwd(jacrev(f,index_derivation),index_derivation)\n",
    "\n",
    "def pde_rhs(t_, x_, params_):\n",
    "    u_out = u(t_, x_, params_)\n",
    "    lap_u = np.trace(np.squeeze(hessian(u,1)(t_, x_, params_)))\n",
    "    return D*lap_u - r*u_out*(1-u_out)\n",
    "\n",
    "@jit\n",
    "def f(t_, x_, params_):\n",
    "    u_t = grad(u,0)(t_, x_, params_)\n",
    "    f_out = u_t - pde_rhs(t_, x_, params_)\n",
    "    return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(-0.02555121, dtype=float32),\n",
       " DeviceArray([-0.02761158], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u(t_test, x_test, init_params), f(t_test, x_test, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u0(x):\n",
    "    return np.clip(10 *np.exp(-vmap(np.dot)(x-0.5,x-0.5)*100), 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(batches, params_, delta = 1e-1, loss_weights={'u': 1.0/3, 'f': 1.0/3, 'delta': 1.0/3}):\n",
    "    t_, x_, u_, tf_, xf_ = batches\n",
    "    \n",
    "    # Physics with mse_f\n",
    "    mse_f = lambda t,x: partial(f, params_=params_)(t,x)**2\n",
    "    v_mse_f = vmap(mse_f, (0,0), 0)\n",
    "    loss_f = np.mean(v_mse_f(tf_, xf_))\n",
    "    \n",
    "    # Delta physics\n",
    "    def mse_delta(t_, x_, params_):\n",
    "        return (u(t_ + delta, x_, params_) - u(t_, x_, params_) - delta * pde_rhs(t_, x_, params_))**2\n",
    "    v_mse_delta = vmap(partial(mse_delta, params_=params_), (0,0), 0)\n",
    "    loss_delta = .5 * (np.mean(v_mse_delta(t_, x_)) + np.mean(v_mse_delta(tf_, xf_)))\n",
    "\n",
    "    # Borders with mse_u\n",
    "    def mse_u(t_, x_, u_, params_):\n",
    "        return np.squeeze((u_ - u(t_, x_, params_))**2)\n",
    "    v_mse_u = vmap(partial(mse_u, params_=params_), (0,0,0), 0)\n",
    "    loss_u = np.mean(v_mse_u(t_, x_, u_))\n",
    "    \n",
    "    # total loss, then aux loss values. Only the first output is differentiated (because of has_aux=True below)\n",
    "    total_loss = loss_weights['u'] * loss_u + loss_weights['f'] * loss_f + loss_weights['delta'] * loss_delta\n",
    "    return (total_loss, (loss_u, loss_f, loss_delta))\n",
    "\n",
    "losses_and_grad = jit(jax.value_and_grad(loss, 1, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 0.054, mse_u: 0.160, mse_f: 0.001, mse_delta: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "# Testing the loss function\n",
    "losses, grads = losses_and_grad((np.zeros((10, 1)), \n",
    "                                 np.zeros((10, 2)), \n",
    "                                 np.ones((10, 1))*0.4, \n",
    "                                 np.ones((10, 1))*0.25,\n",
    "                                 np.ones((10, 2))*0.25),\n",
    "                                 init_params)\n",
    "\n",
    "\n",
    "a, (b, c, d) = losses\n",
    "print(f\"total loss: {a:.3f}, mse_u: {b:.3f}, mse_f: {c:.3f}, mse_delta: {d:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and learning\n",
    "\n",
    "We build $N_u = 100$ boundary data points as mentionned in the paper. Half of them for $t=0$, the other half for $x= \\pm 1$. Wrap it into a dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import datasets\n",
    "\n",
    "key, subkey = random.split(key, 2)\n",
    "ds = datasets.KPPDataset(subkey, u0, batch_size=128, N_u=128*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import optax\n",
    "key, subkey = random.split(key, 2)\n",
    "params = model.init(subkey, t_test, x_test)\n",
    "tx = optax.adam(learning_rate=0.0003)\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 100: 0.0856 / mse_u: -1.3749 / mse_f: -2.9086 /  mse_delta: -20.427872\n",
      "Loss at step 200: 0.0196 / mse_u: -2.0638 / mse_f: -2.6281 /  mse_delta: -19.878386\n",
      "Loss at step 300: 0.0104 / mse_u: -2.4322 / mse_f: -2.5167 /  mse_delta: -19.684092\n",
      "Loss at step 400: 0.0092 / mse_u: -2.4754 / mse_f: -2.6067 /  mse_delta: -19.088905\n",
      "Loss at step 500: 0.0022 / mse_u: -3.4203 / mse_f: -2.8520 /  mse_delta: -19.861916\n",
      "Loss at step 600: 0.0061 / mse_u: -2.6042 / mse_f: -2.9377 /  mse_delta: -19.744234\n",
      "Loss at step 700: 0.0047 / mse_u: -2.7359 / mse_f: -2.9983 /  mse_delta: -20.061142\n",
      "Loss at step 800: 0.0034 / mse_u: -2.9913 / mse_f: -2.8641 /  mse_delta: -19.517435\n",
      "Loss at step 900: 0.0019 / mse_u: -3.7133 / mse_f: -2.8106 /  mse_delta: -19.844921\n",
      "Loss at step 1000: 0.0032 / mse_u: -2.9441 / mse_f: -3.0495 /  mse_delta: -19.740261\n",
      "Loss at step 1100: 0.0015 / mse_u: -3.8653 / mse_f: -2.9176 /  mse_delta: -20.290577\n",
      "Loss at step 1200: 0.0016 / mse_u: -3.3585 / mse_f: -3.1667 /  mse_delta: -19.708570\n",
      "Loss at step 1300: 0.0019 / mse_u: -3.1529 / mse_f: -3.3323 /  mse_delta: -19.771446\n",
      "Loss at step 1400: 0.0015 / mse_u: -3.6864 / mse_f: -2.9734 /  mse_delta: -20.598053\n",
      "Loss at step 1500: 0.0024 / mse_u: -3.1363 / mse_f: -3.0414 /  mse_delta: -19.920017\n",
      "Loss at step 1600: 0.0012 / mse_u: -3.7662 / mse_f: -3.0542 /  mse_delta: -20.805170\n",
      "Loss at step 1700: 0.0012 / mse_u: -3.3855 / mse_f: -3.4352 /  mse_delta: -20.214344\n",
      "Loss at step 1800: 0.0018 / mse_u: -3.3448 / mse_f: -3.0487 /  mse_delta: -20.720854\n",
      "Loss at step 1900: 0.0011 / mse_u: -3.5574 / mse_f: -3.3036 /  mse_delta: -20.554684\n",
      "Loss at step 2000: 0.0011 / mse_u: -3.8669 / mse_f: -3.1026 /  mse_delta: -20.320009\n",
      "Loss at step 2100: 0.0025 / mse_u: -3.0106 / mse_f: -3.3007 /  mse_delta: -20.363060\n",
      "Loss at step 2200: 0.0017 / mse_u: -3.2291 / mse_f: -3.2616 /  mse_delta: -20.533918\n",
      "Loss at step 2300: 0.0010 / mse_u: -3.6604 / mse_f: -3.2165 /  mse_delta: -20.906475\n",
      "Loss at step 2400: 0.0021 / mse_u: -3.1147 / mse_f: -3.2857 /  mse_delta: -20.987610\n",
      "Loss at step 2500: 0.0008 / mse_u: -3.6650 / mse_f: -3.4814 /  mse_delta: -21.491152\n",
      "Loss at step 2600: 0.0012 / mse_u: -3.4876 / mse_f: -3.2964 /  mse_delta: -21.267899\n",
      "Loss at step 2700: 0.0018 / mse_u: -3.2290 / mse_f: -3.2433 /  mse_delta: -20.472727\n",
      "Loss at step 2800: 0.0005 / mse_u: -3.9821 / mse_f: -3.5188 /  mse_delta: -21.630424\n",
      "Loss at step 2900: 0.0007 / mse_u: -4.4988 / mse_f: -3.1995 /  mse_delta: -21.516697\n",
      "Loss at step 3000: 0.0015 / mse_u: -3.2991 / mse_f: -3.2853 /  mse_delta: -20.578268\n",
      "Loss at step 3100: 0.0011 / mse_u: -3.4497 / mse_f: -3.4229 /  mse_delta: -21.544294\n",
      "Loss at step 3200: 0.0011 / mse_u: -3.5125 / mse_f: -3.2980 /  mse_delta: -20.405676\n",
      "Loss at step 3300: 0.0014 / mse_u: -3.3696 / mse_f: -3.2764 /  mse_delta: -21.543543\n",
      "Loss at step 3400: 0.0008 / mse_u: -3.7615 / mse_f: -3.3825 /  mse_delta: -19.994946\n",
      "Loss at step 3500: 0.0013 / mse_u: -3.3109 / mse_f: -3.5474 /  mse_delta: -20.162834\n",
      "Loss at step 3600: 0.0005 / mse_u: -4.0423 / mse_f: -3.4530 /  mse_delta: -20.354645\n",
      "Loss at step 3700: 0.0007 / mse_u: -3.6882 / mse_f: -3.5167 /  mse_delta: -21.280630\n",
      "Loss at step 3800: 0.0003 / mse_u: -4.8098 / mse_f: -3.5529 /  mse_delta: -22.123186\n",
      "Loss at step 3900: 0.0008 / mse_u: -3.7578 / mse_f: -3.3504 /  mse_delta: -20.311981\n",
      "Loss at step 4000: 0.0006 / mse_u: -3.8026 / mse_f: -3.5419 /  mse_delta: -20.558870\n",
      "Loss at step 4100: 0.0004 / mse_u: -4.6496 / mse_f: -3.4000 /  mse_delta: -21.176565\n",
      "Loss at step 4200: 0.0005 / mse_u: -3.8492 / mse_f: -3.7398 /  mse_delta: -20.569603\n",
      "Loss at step 4300: 0.0005 / mse_u: -3.9818 / mse_f: -3.6106 /  mse_delta: -21.419161\n",
      "Loss at step 4400: 0.0005 / mse_u: -4.0224 / mse_f: -3.5441 /  mse_delta: -21.159349\n",
      "Loss at step 4500: 0.0004 / mse_u: -4.1173 / mse_f: -3.5410 /  mse_delta: -20.924471\n",
      "Loss at step 4600: 0.0002 / mse_u: -4.3755 / mse_f: -3.8067 /  mse_delta: -21.871803\n",
      "Loss at step 4700: 0.0006 / mse_u: -3.8707 / mse_f: -3.5483 /  mse_delta: -20.696430\n",
      "Loss at step 4800: 0.0003 / mse_u: -4.3357 / mse_f: -3.6155 /  mse_delta: -20.570736\n",
      "Loss at step 4900: 0.0004 / mse_u: -4.1176 / mse_f: -3.6928 /  mse_delta: -21.910114\n",
      "Loss at step 5000: 0.0004 / mse_u: -4.1619 / mse_f: -3.5572 /  mse_delta: -21.967358\n",
      "Loss at step 5100: 0.0006 / mse_u: -3.6822 / mse_f: -3.7663 /  mse_delta: -20.359097\n",
      "Loss at step 5200: 0.0004 / mse_u: -3.9444 / mse_f: -3.6586 /  mse_delta: -20.650789\n",
      "Loss at step 5300: 0.0005 / mse_u: -3.9111 / mse_f: -3.5824 /  mse_delta: -21.742052\n",
      "Loss at step 5400: 0.0003 / mse_u: -4.3367 / mse_f: -3.7036 /  mse_delta: -20.930962\n",
      "Loss at step 5500: 0.0009 / mse_u: -3.4164 / mse_f: -3.7482 /  mse_delta: -20.821041\n",
      "Loss at step 5600: 0.0002 / mse_u: -4.4176 / mse_f: -3.9523 /  mse_delta: -20.972889\n",
      "Loss at step 5700: 0.0002 / mse_u: -4.8871 / mse_f: -3.6798 /  mse_delta: -21.041544\n",
      "Loss at step 5800: 0.0003 / mse_u: -4.4052 / mse_f: -3.7316 /  mse_delta: -21.328922\n",
      "Loss at step 5900: 0.0003 / mse_u: -4.2189 / mse_f: -3.8011 /  mse_delta: -20.656416\n",
      "Loss at step 6000: 0.0004 / mse_u: -4.0104 / mse_f: -3.6146 /  mse_delta: -21.855019\n",
      "Loss at step 6100: 0.0004 / mse_u: -4.0738 / mse_f: -3.6012 /  mse_delta: -20.781998\n",
      "Loss at step 6200: 0.0004 / mse_u: -4.2649 / mse_f: -3.5919 /  mse_delta: -20.855534\n",
      "Loss at step 6300: 0.0004 / mse_u: -3.8833 / mse_f: -3.7578 /  mse_delta: -21.770185\n",
      "Loss at step 6400: 0.0003 / mse_u: -4.3037 / mse_f: -3.6686 /  mse_delta: -20.838686\n",
      "Loss at step 6500: 0.0003 / mse_u: -4.7953 / mse_f: -3.6035 /  mse_delta: -22.145702\n",
      "Loss at step 6600: 0.0002 / mse_u: -4.6214 / mse_f: -3.9194 /  mse_delta: -21.437023\n",
      "Loss at step 6700: 0.0002 / mse_u: -4.5317 / mse_f: -3.7970 /  mse_delta: -22.459494\n",
      "Loss at step 6800: 0.0002 / mse_u: -4.6091 / mse_f: -3.8254 /  mse_delta: -22.643637\n",
      "Loss at step 6900: 0.0002 / mse_u: -5.0609 / mse_f: -3.6791 /  mse_delta: -21.500887\n",
      "Loss at step 7000: 0.0001 / mse_u: -4.7628 / mse_f: -4.0048 /  mse_delta: -20.900698\n"
     ]
    }
   ],
   "source": [
    "# Main train loop\n",
    "steps = 10000\n",
    "losses_total, losses_u, losses_f, losses_delta = [], [], [], []\n",
    "\n",
    "for i in range(steps):\n",
    "    key, subkey1, subkey2 = random.split(key, 3)\n",
    "    tb, xb, ub = ds.border_batch(subkey1)\n",
    "    tb_uni, xb_uni = ds.inside_batch(subkey2)\n",
    "\n",
    "    losses, grads = losses_and_grad((tb, xb, ub, tb_uni, xb_uni), \n",
    "                                    params,\n",
    "                                    loss_weights={'u': 2, 'f': 1, 'delta': 1})\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    total_loss_val, (mse_u_val, mse_f_val, mse_delta_val) = losses\n",
    "    losses_total.append(total_loss_val)\n",
    "    losses_u.append(mse_u_val)\n",
    "    losses_f.append(mse_f_val)\n",
    "    losses_delta.append(mse_delta_val)    \n",
    "    if i % 100 == 99:\n",
    "        print(f'Loss at step {i+1}: {total_loss_val:.4f} / mse_u: {np.log10(mse_u_val):.4f} / mse_f: {np.log10(mse_f_val):.4f} /  mse_delta: {np.log(mse_delta_val):.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log10(np.array(losses_total)), label=\"total\")\n",
    "plt.plot(np.log10(np.array(losses_u)), label=\"mse_u\")\n",
    "plt.plot(np.log10(np.array(losses_f)), label=\"mse_f\")\n",
    "plt.plot(np.log10(np.array(losses_delta)), label=\"mse_delta\")\n",
    "#plt.plot(losses_f, label=\"mse_f\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_u = vmap(partial(u, params_=params), (0, 0), 0)\n",
    "batched_u0 = lambda t,x: u0(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_KPP_at_times(batched_u, 30, times=[0.0,0.01,0.1,0.2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the current params\n",
    "super_params = params.unfreeze().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

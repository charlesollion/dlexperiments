{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle KPP\n",
    "\n",
    "Tools: jax (to install `pip install jax jaxlib flax`)\n",
    "If you are unfamiliar with Jax random generation, check [this](https://jax.readthedocs.io/en/latest/jax.random.html)\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{eq:KPP_homog}\n",
    "  \\partial_t u(t,x) = D \\Delta u + r u (1 - u), \\ t>0, \\ x\\in \\Omega \\subset \\mathbb{R}^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "avec la condition initiale $u(0,\\cdot)=u_0(\\cdot)$ une gausienne dans $\\Omega$ très \"peakée\", et la condition au bord $u(t,\\cdot )=0$ sur $\\partial\\Omega$ pour tout $t>0$. On pourra prendre $\\Omega=(0,1)\\times(0,1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "from models.nets import MLP\n",
    "from functools import partial\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "# A test point\n",
    "x_test = np.ones(2) * 0.25\n",
    "t_test = np.ones(1) * 0.25\n",
    "\n",
    "model = MLP(features=[20,40,80,80,80,40,20, 1])\n",
    "init_params = model.init(subkey, t_test, x_test)\n",
    "\n",
    "@jit\n",
    "def u(t, x, params_):\n",
    "    return model.apply(params_, t, x)[0]\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_map(np.shape, init_params))\n",
    "print(f'\\nu(x, t): {u(t_test, x_test, init_params):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1.0\n",
    "r = 1.0\n",
    "\n",
    "def hessian(f, index_derivation=0):\n",
    "    return jacfwd(jacrev(f,index_derivation),index_derivation)\n",
    "\n",
    "def pde_rhs(t_, x_, params_):\n",
    "    u_out = u(t_, x_, params_)\n",
    "    lap_u = np.trace(np.squeeze(hessian(u,1)(t_, x_, params_)))\n",
    "    return D*lap_u - r*u_out*(1-u_out)\n",
    "\n",
    "@jit\n",
    "def f(t_, x_, params_):\n",
    "    u_t = grad(u,0)(t_, x_, params_)\n",
    "    f_out = u_t - pde_rhs(t_, x_, params_)\n",
    "    return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u(t_test, x_test, init_params), f(t_test, x_test, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u0(x):\n",
    "    return np.clip(10 *np.exp(-vmap(np.dot)(x-0.5,x-0.5)*100), 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(batches, params_, delta = 1e-1, loss_weights={'u': 1.0/3, 'f': 1.0/3, 'delta': 1.0/3}):\n",
    "    t_, x_, u_, tf_, xf_ = batches\n",
    "    \n",
    "    # Physics with mse_f\n",
    "    mse_f = lambda t,x: partial(f, params_=params_)(t,x)**2\n",
    "    v_mse_f = vmap(mse_f, (0,0), 0)\n",
    "    loss_f = np.mean(v_mse_f(tf_, xf_))\n",
    "    \n",
    "    # Delta physics\n",
    "    def mse_delta(t_, x_, params_):\n",
    "        return (u(t_ + delta, x_, params_) - u(t_, x_, params_) - delta * pde_rhs(t_, x_, params_))**2\n",
    "    v_mse_delta = vmap(partial(mse_delta, params_=params_), (0,0), 0)\n",
    "    loss_delta = .5 * (np.mean(v_mse_delta(t_, x_)) + np.mean(v_mse_delta(tf_, xf_)))\n",
    "\n",
    "    # Borders with mse_u\n",
    "    def mse_u(t_, x_, u_, params_):\n",
    "        return np.squeeze((u_ - u(t_, x_, params_))**2)\n",
    "    v_mse_u = vmap(partial(mse_u, params_=params_), (0,0,0), 0)\n",
    "    loss_u = np.mean(v_mse_u(t_, x_, u_))\n",
    "    \n",
    "    # total loss, then aux loss values. Only the first output is differentiated (because of has_aux=True below)\n",
    "    total_loss = loss_weights['u'] * loss_u + loss_weights['f'] * loss_f + loss_weights['delta'] * loss_delta\n",
    "    return (total_loss, (loss_u, loss_f, loss_delta))\n",
    "\n",
    "losses_and_grad = jit(jax.value_and_grad(loss, 1, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the loss function\n",
    "losses, grads = losses_and_grad((np.zeros((10, 1)), \n",
    "                                 np.zeros((10, 2)), \n",
    "                                 np.ones((10, 1))*0.4, \n",
    "                                 np.ones((10, 1))*0.25,\n",
    "                                 np.ones((10, 2))*0.25),\n",
    "                                 init_params)\n",
    "\n",
    "\n",
    "a, (b, c, d) = losses\n",
    "print(f\"total loss: {a:.3f}, mse_u: {b:.3f}, mse_f: {c:.3f}, mse_delta: {d:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and learning\n",
    "\n",
    "We build $N_u = 100$ boundary data points as mentionned in the paper. Half of them for $t=0$, the other half for $x= \\pm 1$. Wrap it into a dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import datasets\n",
    "\n",
    "key, subkey = random.split(key, 2)\n",
    "ds = datasets.KPPDataset(subkey, u0, batch_size=128, N_u=128*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import optax\n",
    "key, subkey = random.split(key, 2)\n",
    "params = model.init(subkey, t_test, x_test)\n",
    "tx = optax.adam(learning_rate=0.0003)\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main train loop\n",
    "steps = 3000\n",
    "losses_total, losses_u, losses_f, losses_delta = [], [], [], []\n",
    "\n",
    "for i in range(steps):\n",
    "    key, subkey1, subkey2 = random.split(key, 3)\n",
    "    tb, xb, ub = ds.border_batch(subkey1)\n",
    "    tb_uni, xb_uni = ds.inside_batch(subkey2)\n",
    "\n",
    "    losses, grads = losses_and_grad((tb, xb, ub, tb_uni, xb_uni), \n",
    "                                    params,\n",
    "                                    loss_weights={'u': 2, 'f': 1, 'delta': 1})\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    total_loss_val, (mse_u_val, mse_f_val, mse_delta_val) = losses\n",
    "    losses_total.append(total_loss_val)\n",
    "    losses_u.append(mse_u_val)\n",
    "    losses_f.append(mse_f_val)\n",
    "    losses_delta.append(mse_delta_val)    \n",
    "    if i % 100 == 99:\n",
    "        print(f'Loss at step {i+1}: {total_loss_val:.4f} / mse_u: {np.log10(mse_u_val):.4f} / mse_f: {np.log10(mse_f_val):.4f} /  mse_delta: {np.log(mse_delta_val):.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log10(np.array(losses_total)), label=\"total\")\n",
    "plt.plot(np.log10(np.array(losses_u)), label=\"mse_u\")\n",
    "plt.plot(np.log10(np.array(losses_f)), label=\"mse_f\")\n",
    "plt.plot(np.log10(np.array(losses_delta)), label=\"mse_delta\")\n",
    "#plt.plot(losses_f, label=\"mse_f\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_u = vmap(partial(u, params_=params), (0, 0), 0)\n",
    "batched_u0 = lambda t,x: u0(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.display import display_KPP_at_times\n",
    "# display_KPP_at_times(batched_u0, 30, times=[0.0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_KPP_at_times(batched_u, 30, times=[0.0,0.01,0.1,0.2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the current params\n",
    "super_params = params.unfreeze().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPP avec Solver standard discretisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from models.solver import build_init_grid\n",
    "\n",
    "Nx = Ny = 128\n",
    "def u0(x, y):\n",
    "    return 10 *np.exp(-((x-0.5)**2 + (y-0.5)**2)*100)\n",
    "\n",
    "xx, yy, uu = build_init_grid(u0, Nx, Ny)\n",
    "uu = uu/np.max(uu)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(uu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perlin_numpy import generate_perlin_noise_2d\n",
    "rgrid = generate_perlin_noise_2d((Nx,Ny),(Nx//16,Ny//16)) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r0(x,y):\n",
    "    return 4 *np.exp(-((x-0.25)**2 + (y-0.75)**2)*5)\n",
    "\n",
    "#rgrid = r0(xx, yy)\n",
    "plt.imshow(rgrid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "from models.solver import solver_iter\n",
    "\n",
    "Nt = 300\n",
    "grid = uu.copy()\n",
    "plt.imshow(uu);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Nt * 2):\n",
    "    grid = solver_iter(grid, 1./Nt, rgrid)\n",
    "plt.imshow(grid);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(uu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grid - uu);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPP avec NN discretisé\n",
    "\n",
    "- [ ] Discretisation sur le temps: $u(x)$ est un vecteur de fonctions de $x \\in \\mathbb{R}^2$, de $T=64$ timesteps.\n",
    "- [x] Discretisation sur l'espace: $u(t, u0)$ est une grille de taille $\\Omega=128 \\times 128$.\n",
    "- [ ] Double discretisation $u$ est un volume de taille $T \\times \\Omega = 64 \\times 32 \\times 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nets import UNet\n",
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "key = random.PRNGKey(1)\n",
    "key, subkey = random.split(key)\n",
    "unet = UNet(features=64, training=False)\n",
    "\n",
    "init_rngs = {'params': random.PRNGKey(0)}\n",
    "\n",
    "unet_variables = unet.init(init_rngs, np.ones([1, 128, 128, 1]), np.ones([1]))\n",
    "\n",
    "@jit\n",
    "def unet_func(params, x, t):\n",
    "    return unet.apply({\"params\":params}, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0_batched = np.expand_dims(uu, (0, -1))\n",
    "t_test = np.array([[0.0]])\n",
    "out_test = unet_func(unet_variables[\"params\"], u0_batched, t_test)\n",
    "print(out_test.shape)\n",
    "plt.imshow(np.squeeze(out_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(u0_batched));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def jac(params, f, t, x):\n",
    "    unet_t = partial(f, params, x)\n",
    "    out = np.squeeze(jacfwd(unet_t)(t), (-1,-2))\n",
    "    return out\n",
    "\n",
    "jac(unet_variables[\"params\"], unet_func, t_test, u0_batched).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.jaxtools import laplacian_grid\n",
    "D = 1.0\n",
    "r = 1.0\n",
    "\n",
    "@jit\n",
    "def mse_phy(params, t_, x_):\n",
    "    # time derivative\n",
    "    du_t = jac(params, unet_func, t_, x_)\n",
    "    # rhs\n",
    "    u_out = unet_func(params, x_, t_)\n",
    "    lap_u = np.expand_dims(laplacian_grid(np.squeeze(u_out)), (0, -1))\n",
    "    pde_rhs = D*lap_u - r*u_out*(1-u_out)\n",
    "    f_out = du_t - pde_rhs\n",
    "    return np.mean(f_out**2)\n",
    "\n",
    "mask_border = np.expand_dims(1. - np.pad(np.ones((Nx-2, Ny-2)), ((1, 1), (1,1))), (0, -1))\n",
    "\n",
    "@jit\n",
    "def mse_border(params, t_, x_):\n",
    "    u_out = unet_func(params, x_, t_)\n",
    "    return np.mean(u_out **2 * mask_border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mse_phy(unet_variables[\"params\"], t_test, u0_batched), \n",
    " mse_border(unet_variables[\"params\"], t_test, u0_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(t_, params_, delta = 1e-1, loss_weights={'border': 1., 'phy': 1.}):\n",
    "    loss_border, loss_phy = mse_phy(params_, t_, u0_batched), mse_border(params_, t_, u0_batched)\n",
    "    total_loss = loss_weights['border'] * loss_border + loss_weights['phy'] * loss_phy\n",
    "    \n",
    "    # total loss, then aux loss values. Only the first output is differentiated (because of has_aux=True below)\n",
    "    return (total_loss, (loss_border, loss_phy))\n",
    "\n",
    "losses_and_grad = jit(jax.value_and_grad(loss, 1, has_aux=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: we verify that we are able to take the gradient of the loss with regards to the unet params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = grad(loss, 1, has_aux=True)(t_test, unet_variables[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the loss function\n",
    "key, subkey = random.split(key)\n",
    "t = random.uniform(subkey, shape=(1,1))\n",
    "\n",
    "losses, grads = losses_and_grad(t, unet_variables[\"params\"])\n",
    "\n",
    "a, (b, c) = losses\n",
    "print(f\"total loss: {a:.3f}, mse_border: {b:.3f}, mse_phy: {c:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    cnn = UNet()\n",
    "    params = cnn.init(rng, np.ones([1, 128, 128, 1]), np.ones([1]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, key):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    key, subkey = random.split(key)\n",
    "    t = random.uniform(subkey, shape=(1,1))\n",
    "\n",
    "    (_, logits), grads = losses_and_grad(t, state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(subkey, 0.01, 0.99)\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_iter(state, n_iter, rng):\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        state = train_step(state, rng)\n",
    "        \n",
    "        \n",
    "train_iter(state, 10, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0.0\n",
    "out_test = unet_func(state.params, u0_batched, np.ones([1])* time)\n",
    "plt.imshow(np.squeeze(out_test));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

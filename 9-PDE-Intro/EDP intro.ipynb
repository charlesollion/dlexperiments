{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burger's equation\n",
    "\n",
    "Tools: jax (to install `pip install jax jaxlib flax`)\n",
    "\n",
    "Goal: have a first simple 1D model to work with.\n",
    "\n",
    "https://arxiv.org/pdf/1711.10561.pdf\n",
    "\n",
    "Burger's equation becomes:\n",
    "$$\n",
    "u_t + u \\times u_x − (0.01/π)u_{xx} = 0, x ∈ [−1, 1], t ∈ [0, 1], \\\\\n",
    "u(0, x) = − sin(πx), \\\\\n",
    "u(t, −1) = u(t, 1) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import Sequence\n",
    "\n",
    "# Jax uses a state based random number generation \n",
    "# process, which is much less error prone than (hidden) \n",
    "# stateless cases in tensorflow, pytorch, but more \n",
    "# cumbersome. You need to split the key into 2 parts \n",
    "# and use subparts to generate your random numbers.\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        h = np.concatenate((t,x))\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "x = np.zeros((1,))\n",
    "t = np.zeros((1,))\n",
    "\n",
    "model = MLP(features=[3,4,5,1])\n",
    "params = model.init(subkey, t, x)\n",
    "y = model.apply(params, t, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_map(np.shape, unfreeze(params)))\n",
    "print('output:\\n', y)\n",
    "\n",
    "def u(t, x, params):\n",
    "    return model.apply(params, t, x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 0 border condition\n",
    "def u0(x):\n",
    "    return - np.sin(np.pi * x)\n",
    "\n",
    "# u_xx\n",
    "def hessian(f, index_derivation=0):\n",
    "    return jacfwd(jacrev(f,index_derivation),index_derivation)\n",
    "\n",
    "@jit\n",
    "def f(t_, x_, params_):\n",
    "    u_out = u(t_, x_, params_)\n",
    "    u_t = grad(u,0)(t_, x_, params_)\n",
    "    u_x = grad(u,1)(t_, x_, params_)\n",
    "    u_xx = hessian(u, 1)(t_, x_, params_)\n",
    "    f_out = u_t + u_out*u_x - (0.01/np.pi)*u_xx\n",
    "    return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test point\n",
    "x_test = np.ones(1) * 0.25\n",
    "t_test = np.ones(1) * 0.25\n",
    "\n",
    "u(t_test, x_test, params), f(t_test, x_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_f(t_, x_, params_):\n",
    "    return np.mean(f(t_, x_, params_)**2)\n",
    "\n",
    "mse_f(t_test, x_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of vmap to batch efficiently\n",
    "batched_mse_f = vmap(mse_f, (0, 0, None), 0)\n",
    "\n",
    "def loss_f(t_, x_, params_):\n",
    "    return np.sum(batched_mse_f(t_, x_, params_))\n",
    "\n",
    "loss_f(np.zeros((10, 1)), \n",
    "       np.zeros((10, 1)),  \n",
    "       params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_u(t_, x_, u_, params_):\n",
    "    return np.mean((u_ - u(t_, x_, params_))**2)\n",
    "\n",
    "mse_u(np.zeros(1), np.zeros(1), np.zeros(1), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of vmap to batch efficiently\n",
    "batched_mse_u = vmap(mse_u, (0, 0, 0, None), 0)\n",
    "\n",
    "def loss(t_, x_, u_, params_):\n",
    "    return np.sum(batched_mse_u(t_, x_, u_, params_))\n",
    "\n",
    "loss(np.zeros((10, 1)), \n",
    "     np.zeros((10, 1)), \n",
    "     np.zeros((10, 1)), \n",
    "     params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total loss, todo\n",
    "def mse(params_, t_u_, x_u_, u_, t_f_, x_f_):\n",
    "    return jit(mse_u)(t_u_, x_u_, u_, params_) + jit(mse_f)(t_f_, x_f_, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "# Will output 2 objects: loss and gradients wrt to every parameters\n",
    "loss_grad_fn(params, np.ones(1), np.zeros(1), np.zeros(1), np.ones(1), np.zeros(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and learning\n",
    "\n",
    "We build $N_u = 100$ boundary data points as mentionned in the paper. Half of them for $t=0$, the other half for $x= \\pm 1$. Wrap it into a dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, key, batch_size=10, N_u=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.N_u = N_u\n",
    "        self.curr_idx = 0\n",
    "        x_,t_,u_ = self.generate_data(key, N_u)\n",
    "        self.x_data = x_\n",
    "        self.t_data = t_\n",
    "        self.u_data = u_\n",
    "        \n",
    "        \n",
    "    def generate_data(self, key, N_u):\n",
    "        key, subkey = random.split(key)\n",
    "        data_type = random.uniform(key, (N_u,))>0.5\n",
    "        key, subkey1, subkey2, subkey3 = random.split(key, 4)\n",
    "        x_data = data_type * (random.uniform(subkey1, (N_u,))*2.0-1.0)+ \\\n",
    "            (1-data_type) * ((random.uniform(subkey2, (N_u,))>0.5)*2.0-1.0)\n",
    "        t_data = data_type * 0 + ((1-data_type) * random.uniform(subkey3, (N_u,)))\n",
    "        u_data = data_type * u0(x_data)\n",
    "        return x_data, t_data, u_data\n",
    "\n",
    "        \n",
    "    def next_batch(self):\n",
    "        bstart = self.curr_idx * self.batch_size\n",
    "        bend = (self.curr_idx + 1) * self.batch_size\n",
    "        if bend >= self.N_u:\n",
    "            bend = self.N_u-1\n",
    "            self.curr_idx = 0\n",
    "        else:\n",
    "            self.curr_idx = self.curr_idx + 1\n",
    "        x_ = np.expand_dims(self.x_data[bstart:bend], -1)\n",
    "        t_ = np.expand_dims(self.t_data[bstart:bend], -1)\n",
    "        u_ = np.expand_dims(self.u_data[bstart:bend], -1)\n",
    "        return t_, x_, u_\n",
    "    \n",
    "    def uniform_batch(self, key):\n",
    "        key1, key2 = random.split(key, 2)\n",
    "        tb = random.uniform(key1, (self.batch_size, 1))\n",
    "        xb = random.uniform(key2, (self.batch_size, 1))\n",
    "        return tb, xb\n",
    "    \n",
    "ds = Dataset(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "import optax\n",
    "key, subkey = random.split(key, 2)\n",
    "params = model.init(subkey, t, x)\n",
    "tx = optax.sgd(learning_rate=0.05)\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "batch = 10\n",
    "loss_grad_fn = jit(jax.value_and_grad(loss, 3))\n",
    "loss_grad_f_fn = jit(jax.value_and_grad(loss_f, 2))\n",
    "\n",
    "# Main train loop\n",
    "for i in range(steps):\n",
    "    tb, xb, ub = ds.next_batch()\n",
    "    loss_val, grads = loss_grad_fn(tb, xb, ub, params)\n",
    "\n",
    "    # f_loss: not used for now\n",
    "    key, subkey = random.split(key, 2)\n",
    "    tb_uni, xb_uni = ds.uniform_batch(subkey)\n",
    "    loss_f_val, grads_f = loss_grad_f_fn(tb_uni, xb_uni, params)\n",
    "    \n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    #updates, opt_state = tx.update(grads_f, opt_state)\n",
    "    #params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: mse_u: {loss_val:.2f} / mse_f: {loss_f_val:.2f}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grads_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u(np.zeros(1,), x_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_u(resolution):\n",
    "    nt, nx = resolution,resolution\n",
    "    t = np.linspace(0, 1, nt)\n",
    "    x = np.linspace(-1, 1, nx)\n",
    "    print(t.shape, x.shape)\n",
    "    tv, xv = np.meshgrid(t, x)\n",
    "    tv = tv.reshape((nt * nx, 1))\n",
    "    xv = xv.reshape((nt * nx, 1))\n",
    "\n",
    "    batched_u = vmap(u, (0, 0, None), 0)\n",
    "    values = batched_u(tv, xv, params)\n",
    "    grid = values.reshape(nt, nx)\n",
    "    plt.imshow(grid)\n",
    "    plt.xticks([x*resolution/5 for x in range(5)], [round(i/(1.0*5), 2) for i in range(5)])\n",
    "    plt.yticks([x*resolution/5 for x in range(5)], [round(1.0-2*i/(5), 2) for i in range(5)])\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_u(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: \n",
    "- sample $N_f$ points for evaluation of $f$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle KPP\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{eq:KPP_homog}\n",
    "  \\partial_t u(t,x) = D \\Delta u + r u (1 - u), \\ t>0, \\ x\\in \\Omega \\subset \\mathbb{R}^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "avec la condition initiale $u(0,\\cdot)=u_0(\\cdot)$ dans $\\Omega$ et la condition au bord $u(t,\\cdot )=0$ sur $\\partial\\Omega$ pour tout $t>0$. On pourra prendre $\\Omega=(0,1)\\times(0,1)$.\n",
    "\n",
    "To be continued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "# Jax uses a state based random number generation \n",
    "# process, which is much less error prone than (hidden) \n",
    "# stateless cases in tensorflow, pytorch, but more \n",
    "# cumbersome. You need to split the key into 2 parts \n",
    "# and use subparts to generate your random numbers.\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "W = random.normal(subkey, (2, 1))\n",
    "b = np.zeros(1)\n",
    "params = (W, b)\n",
    "\n",
    "# Simple u function: only 3 params!\n",
    "def u(t, x, params):\n",
    "    W, b = params\n",
    "    xandt = np.concatenate((t, x))\n",
    "    return np.dot(xandt, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for noobs: less Numpy way to build data (with loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = 100\n",
    "data = []\n",
    "\n",
    "for i in range(N_u):\n",
    "    x_data,t_data,u_data = 0.0, 0.0, 0.0\n",
    "    key, subkey = random.split(key)\n",
    "    if  random.uniform(subkey)>0.5:\n",
    "        t_data = 0\n",
    "        key, subkey = random.split(key)\n",
    "        x_data = random.uniform(subkey)*2-1\n",
    "        u_data = u0(x_data)\n",
    "        # t=0, u= -sin(pi x)\n",
    "    else:\n",
    "        key, subkey = random.split(key)\n",
    "        t_data = random.uniform(subkey)\n",
    "        key, subkey = random.split(key)\n",
    "        x_data = (random.uniform(subkey)>0.5)*2-1\n",
    "        u_data = 0\n",
    "    data.append([x_data,t_data,u_data])\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

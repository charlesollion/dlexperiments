{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burger's equation\n",
    "\n",
    "Tools: jax (to install `pip install jax jaxlib flax`)\n",
    "If you are unfamiliar with Jax random generation, check [this](https://jax.readthedocs.io/en/latest/jax.random.html)\n",
    "\n",
    "Goal: have a first simple 1D model to work with similar to [this paper](https://arxiv.org/pdf/1711.10561.pdf)\n",
    "\n",
    "\n",
    "\n",
    "Burger's equation becomes:\n",
    "$$\n",
    "u_t + u \\times u_x − (0.01/π)u_{xx} = 0, x ∈ [−1, 1], t ∈ [0, 1], \\\\\n",
    "u(0, x) = − sin(πx), \\\\\n",
    "u(t, −1) = u(t, 1) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "from models.nets import MLP\n",
    "from functools import partial\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "# A test point\n",
    "x_test = np.ones(1) * 0.25\n",
    "t_test = np.ones(1) * 0.25\n",
    "\n",
    "model = MLP(features=[20,20,20,20,20,20,20, 1])\n",
    "init_params = model.init(subkey, t_test, x_test)\n",
    "\n",
    "@jit\n",
    "def u(t, x, params_):\n",
    "    return model.apply(params_, t, x)[0]\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_map(np.shape, init_params))\n",
    "print(f'\\nu(x, t): {u(t_test, x_test, init_params):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 0 border condition\n",
    "def u0(x):\n",
    "    return - np.sin(np.pi * x)\n",
    "\n",
    "# u_xx\n",
    "def hessian(f, index_derivation=0):\n",
    "    return jacfwd(jacrev(f,index_derivation),index_derivation)\n",
    "\n",
    "@jit\n",
    "def f(t, x, params_):\n",
    "    u_out = u(t, x, params_)\n",
    "    u_t = grad(u,0)(t, x, params_)\n",
    "    u_x = grad(u,1)(t, x, params_)\n",
    "    u_xx = hessian(u, 1)(t, x, params_)[0]\n",
    "    f_out = u_t + u_out*u_x - (0.01/np.pi)*u_xx\n",
    "    return np.squeeze(f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our functions\n",
    "u(t_test, x_test, init_params), f(t_test, x_test, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(batches, params_):\n",
    "    t_, x_, u_, tf_, xf_ = batches\n",
    "    \n",
    "    # Physics with mse_f\n",
    "    mse_f = lambda t,x: partial(f, params_=params_)(t,x)**2\n",
    "    v_mse_f = vmap(mse_f, (0,0), 0)\n",
    "    loss_f = np.mean(v_mse_f(tf_, xf_))\n",
    "    \n",
    "    # Borders with mse_u\n",
    "    def mse_u(t_, x_, u_, params_):\n",
    "        return np.mean((u_ - u(t_, x_, params_))**2)\n",
    "    v_mse_u = vmap(partial(mse_u, params_=params_), (0,0,0), 0)\n",
    "    loss_u = np.mean(v_mse_u(t_, x_, u_))\n",
    "    \n",
    "    # total loss, then aux loss values. Only the first output is differentiated (because of has_aux=True below)\n",
    "    return (loss_f+loss_u, (loss_u, loss_f))\n",
    "\n",
    "losses_and_grad = jit(jax.value_and_grad(loss, 1, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the loss function\n",
    "losses, grads = losses_and_grad((np.zeros((10, 1)), \n",
    "                                 np.zeros((10, 1)), \n",
    "                                 np.ones((10, 1))*0.4, \n",
    "                                 np.ones((10, 1))*0.25,\n",
    "                                 np.ones((10, 1))*0.25),\n",
    "                                 init_params)\n",
    "\n",
    "\n",
    "a, (b,c) = losses\n",
    "print(f\"total loss: {a:.3f}, mse_u: {b:.3f}, mse_f: {c:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and learning\n",
    "\n",
    "We build $N_u = 100$ boundary data points as mentionned in the paper. Half of them for $t=0$, the other half for $x= \\pm 1$. Wrap it into a dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import datasets\n",
    "\n",
    "key, subkey = random.split(key, 2)\n",
    "ds = datasets.BurgersDataset(subkey, u0, batch_size=32, N_u=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import optax\n",
    "key, subkey = random.split(key, 2)\n",
    "params = model.init(subkey, t_test, x_test)\n",
    "tx = optax.adam(learning_rate=0.001)\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main train loop\n",
    "steps = 5000\n",
    "for i in range(steps):\n",
    "    tb, xb, ub = ds.border_batch()\n",
    "    tb_uni, xb_uni = ds.inside_batch()\n",
    "    \n",
    "    losses, grads = losses_and_grad((tb, xb, ub, tb_uni, xb_uni), \n",
    "                                    params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    total_loss_val, (mse_u_val, mse_f_val) = losses\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f'Loss at step {i+1}: {total_loss_val:.4f} / mse_u: {mse_u_val:.4f} / mse_f: {mse_f_val:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_u = vmap(partial(u, params_=params), (0, 0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.display import display_burgers_grid, display_burgers_slice\n",
    "\n",
    "display_burgers_grid(batched_u, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_burgers_slice(batched_u, 30, slices=[0.0, 0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle KPP\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{eq:KPP_homog}\n",
    "  \\partial_t u(t,x) = D \\Delta u + r u (1 - u), \\ t>0, \\ x\\in \\Omega \\subset \\mathbb{R}^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "avec la condition initiale $u(0,\\cdot)=u_0(\\cdot)$ dans $\\Omega$ et la condition au bord $u(t,\\cdot )=0$ sur $\\partial\\Omega$ pour tout $t>0$. On pourra prendre $\\Omega=(0,1)\\times(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev\n",
    "from jax import random\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import Sequence\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        h = np.concatenate((t,x))\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "x = np.zeros((2,))\n",
    "t = np.zeros((1,))\n",
    "\n",
    "model = MLP(features=[20,20,1])\n",
    "params = model.init(subkey, t, x)\n",
    "y = model.apply(params, t, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_map(np.shape, unfreeze(params)))\n",
    "print('output:\\n', y)\n",
    "\n",
    "def u(t, x, params):\n",
    "    return model.apply(params, t, x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1.0\n",
    "r = 1.1\n",
    "\n",
    "def hessian(f, index_derivation=0):\n",
    "    return jacfwd(jacrev(f,index_derivation),index_derivation)\n",
    "\n",
    "@jit\n",
    "def f(t_, x_, params_):\n",
    "    u_out = u(t_, x_, params_)\n",
    "    lap_u = np.trace(np.squeeze(hessian(u,1)(t_, x_, params_)))\n",
    "    u_t = grad(u,0)(t_, x_, params_)\n",
    "    f_out = u_t - D*lap_u - r*u_out*(1-u_out)\n",
    "    return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test point\n",
    "x_test = np.ones(2) * 0.25\n",
    "t_test = np.ones(1) * 0.25\n",
    "\n",
    "u(t_test, x_test, params), f(t_test, x_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_f(t_, x_, params_):\n",
    "    return np.mean(f(t_, x_, params_)**2)\n",
    "\n",
    "mse_f(t_test, x_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of vmap to batch efficiently\n",
    "batched_mse_f = vmap(mse_f, (0, 0, None), 0)\n",
    "\n",
    "def loss_f(t_, x_, params_):\n",
    "    return np.mean(batched_mse_f(t_, x_, params_))\n",
    "\n",
    "loss_f(np.zeros((10, 1)), \n",
    "       np.zeros((10, 2)),  \n",
    "       params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_u(t_, x_, u_, params_):\n",
    "    return np.mean((u_ - u(t_, x_, params_))**2)\n",
    "\n",
    "mse_u(np.zeros(1), np.zeros(2), np.zeros(1), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of vmap to batch efficiently\n",
    "batched_mse_u = vmap(mse_u, (0, 0, 0, None), 0)\n",
    "\n",
    "def loss_u(t_, x_, u_, params_):\n",
    "    return np.mean(batched_mse_u(t_, x_, u_, params_))\n",
    "\n",
    "loss_u(np.zeros((10, 1)), \n",
    "     np.zeros((10, 2)), \n",
    "     np.ones((10, 1))*0.4, \n",
    "     params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(t_, x_, u_, t2_, x2_, params_):\n",
    "    return loss_f(t2_, x2_, params_) + loss_u(t_, x_, u_, params_)\n",
    "\n",
    "total_loss(np.zeros((10, 1)), \n",
    "     np.zeros((10, 2)), \n",
    "     np.ones((10, 1))*0.4, \n",
    "     np.ones((10, 1))*0.25,\n",
    "     np.ones((10, 2))*0.25,\n",
    "     params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unused function\n",
    "def add_gradients(grad1, grad2, alpha=1.0, beta=1.0):\n",
    "    return jax.tree_multimap(lambda x, y: x*alpha+y*beta, grads, grads_f)\n",
    "\n",
    "add_grads = jit(add_gradients)\n",
    "\n",
    "loss_grad_fn = jit(jax.value_and_grad(loss_u, 3))\n",
    "loss_grad_f_fn = jit(jax.value_and_grad(loss_f, 2))\n",
    "loss_grad_total_fn = jit(jax.value_and_grad(total_loss, 5))\n",
    "\n",
    "\"\"\"def losses_and_grad(t_, x_, u_, t2_, x2_, params_):\n",
    "    loss_f = loss_f(t2_, x2_, params_)\n",
    "    loss_u = loss_u(t_, x_, u_, params_)\n",
    "    jax.value_and_grad(total_loss, has_aux=True)\n",
    "    grad(total_loss, 5)\n",
    "\n",
    "losses_and_grad_fn = jit(losses_and_grad)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u0(x):\n",
    "    return np.exp(-vmap(np.dot)(x-0.5,x-0.5)*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetKPP():\n",
    "    def __init__(self, key, batch_size=10, N_u=500, N_f=5000):\n",
    "        self.batch_size = batch_size\n",
    "        self.N_u = N_u\n",
    "        self.N_f = N_f\n",
    "        self.curr_idx = 0\n",
    "        self.curr_f_idx = 0\n",
    "        x_,t_,u_ = self.generate_data(key, N_u)\n",
    "        self.x_data = x_\n",
    "        self.t_data = t_\n",
    "        self.u_data = u_\n",
    "        key1, key2 = random.split(key, 2)\n",
    "        self.t_f_data = random.uniform(key1, (self.N_f, 1))\n",
    "        self.x_f_data = random.uniform(key2, (self.N_f, 2))\n",
    "        \n",
    "        \n",
    "    def generate_data(self, key, N_u):\n",
    "        key, subkey = random.split(key)\n",
    "        data_type = random.uniform(key, (N_u,1))>0.5\n",
    "        key, subkey1, subkey2, subkey3 = random.split(key, 4)\n",
    "        x_data = data_type * (random.uniform(subkey1, (N_u,2)))+ \\\n",
    "            (1-data_type) * ((random.uniform(subkey2, (N_u,2))>0.5))\n",
    "        t_data = data_type * 0 + ((1-data_type) * random.uniform(subkey3, (N_u,1)))\n",
    "        u_data = data_type * np.expand_dims(u0(x_data),-1)\n",
    "        return x_data, t_data, u_data\n",
    "\n",
    "        \n",
    "    def next_batch(self):\n",
    "        bstart = self.curr_idx * self.batch_size\n",
    "        bend = (self.curr_idx + 1) * self.batch_size\n",
    "        if bend >= self.N_u:\n",
    "            bend = self.N_u-1\n",
    "            self.curr_idx = 0\n",
    "        else:\n",
    "            self.curr_idx = self.curr_idx + 1\n",
    "        x_ = self.x_data[bstart:bend]\n",
    "        t_ = self.t_data[bstart:bend]\n",
    "        u_ = self.u_data[bstart:bend]\n",
    "        return t_, x_, u_\n",
    "    \n",
    "    def uniform_batch(self, size):\n",
    "        bstart = self.curr_f_idx \n",
    "        bend = bstart + size\n",
    "        if bend >= self.N_f:\n",
    "            bend = self.N_f-1\n",
    "            self.curr_f_idx = 0\n",
    "        else:\n",
    "            self.curr_f_idx = self.curr_f_idx + size\n",
    "        t_b = self.t_f_data[bstart:bend]\n",
    "        x_b = self.x_f_data[bstart:bend]\n",
    "        return t_b, x_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=DatasetKPP(key, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import optax\n",
    "key, subkey = random.split(key, 2)\n",
    "params = model.init(subkey, t, x)\n",
    "tx = optax.adam(learning_rate=0.001)\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "batch_nf = 512\n",
    "# Main train loop\n",
    "for i in range(steps):\n",
    "    \n",
    "    # OLD\n",
    "    #loss_val, grads = loss_grad_fn(tb, xb, ub, params)\n",
    "    #loss_f_val, grads_f = loss_grad_f_fn(tb_uni, xb_uni, params)\n",
    "    #total_grads = add_grads(grads, grads_f, alpha=1.0, beta=1.0)\n",
    "\n",
    "    tb, xb, ub = ds.next_batch()\n",
    "    tb_uni, xb_uni = ds.uniform_batch(batch_nf)\n",
    "    \n",
    "    total_loss, total_grads = loss_grad_total_fn(tb, xb, ub, tb_uni, xb_uni, params)\n",
    "    updates, opt_state = tx.update(total_grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss step {i}: loss: {total_loss:.4f}') \n",
    "    #print(f' / mse_u: {loss_val:.2f} / mse_f: {loss_f_val:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def display_sliceKPP(resolution=30, slices=[0.0,]):\n",
    "    num = len(slices)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    batched_u = vmap(u, (0, 0, None), 0)\n",
    "    batched_u0 = vmap(u0)\n",
    "    for i in range(num):\n",
    "        plt.subplot(1, num,i+1)\n",
    "        tt = np.ones((resolution, 1)) * slices[i]\n",
    "        xx = np.expand_dims(np.linspace(0,1, resolution),-1)\n",
    "        yy = np.expand_dims(np.linspace(0,1, resolution),-1)\n",
    "        map_out=[]\n",
    "        \n",
    "        for ix in range(resolution):\n",
    "            xrow = np.expand_dims(np.repeat(xx[ix], resolution),axis=-1)\n",
    "            row = np.concatenate((xrow,yy), axis=-1)\n",
    "            outp = batched_u(tt, row, params)\n",
    "            #outp = u0(row)\n",
    "            map_out.append(np.expand_dims(outp, axis=-1))\n",
    "        map_out = np.hstack(map_out)\n",
    "        plt.imshow(map_out, vmin=-0.05, vmax=0.2)\n",
    "        plt.xticks([x*resolution/5 for x in range(5)], [round(i/5.0, 2) for i in range(5)])\n",
    "        plt.yticks([x*resolution/5 for x in range(5)], [round(i/5.0, 2) for i in range(5)])\n",
    "\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sliceKPP(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sliceKPP(20, slices=[0, 0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0(np.ones((10,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(np.ones(1),10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan> Variational inference </font>\n",
    "#### <font color=darkorange>Basics: Evidence Lower Bound (ELBO) & Coordinate Ascent Variational Inference (CAVI) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Required packages\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Mixture of Gaussian distributions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following ``Variational Inference: A Review for Statisticians, Blei et al; (2017)``, consider a Bayesian mixture of unit-variance univariate Gaussians. \n",
    "\n",
    "There are $K$ mixture components, each component $k$ of the mixture is a Gaussian distribution with mean $\\mu_k$ and variance 1. The random variables $\\mu = (\\mu_k)_{1\\leqslant k \\leqslant K}$ are assumed to be independent and identically distributed (i.i.d.) with Gaussian distribution with mean 0 and variance $\\sigma^2$. The weight of each mixture component $k$ written $\\omega_k$. Conditionally on $\\mu$, the observations $(X_i)_{1\\leqslant i\\leqslant n}$ are assumed to be i.i.d. with probability density:\n",
    "\n",
    "$$\n",
    "p(x|\\mu) = \\sum_{k=1}^K \\omega_k \\varphi_{\\mu_k,1}(x)\\,,\n",
    "$$\n",
    "\n",
    "where $\\varphi_{\\mu_k,\\sigma^2}$ is the Gaussian probability function with mean $\\mu_k$ and variance $\\sigma^2$. The likelihood is then given by:\n",
    "\n",
    "$$\n",
    "p(x_1,\\cdots,x_n) = \\int p(x_1,\\cdots,x_n|\\mu) p(\\mu) \\mathrm{d} \\mu = \\int \\prod_{i=1}^n p(x_i|\\mu) p(\\mu) \\mathrm{d} \\mu = \\int \\prod_{i=1}^n \\left(\\sum_{k=1}^K \\omega_k \\varphi_{\\mu_k,1}(x_i)\\right) p(\\mu) \\mathrm{d} \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application with $K= 3$, $\\sigma^2 = 1$, $\\omega_k = 1/K$ for all $1\\leqslant k \\leqslant K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "K  = 3 # number of mixture components\n",
    "mu = np.random.normal(0,1,3) # means of the distribution in each cluster\n",
    "n_samples = 1000 # number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,K,n_samples) # labels of each observation\n",
    "X = np.zeros(n_samples)\n",
    "np.random.seed(0)\n",
    "for i in range(n_samples):\n",
    "    X[i] = np.random.normal(loc=mu[idx[i]], scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkcyan> **Note charles**\n",
    "The more numpythonic way of doing is working with arrays directly instead of loops. It's faster, and in many cases will be mandatory for automatic differentiation and fast parallelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,K,n_samples)\n",
    "np.random.seed(0)\n",
    "# here mu[idx], when idx is an integer array, returns an array of the mu values indexed by idx\n",
    "X2 = np.random.normal(0, 1, n_samples) + mu[idx]\n",
    "np.all(X==X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['salmon','seagreen','darkgreen']\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.arange(n_samples), X, c=idx, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Individual')\n",
    "plt.ylabel('X')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to approximate the posterior distribution $p(\\mu,c|x)$ where $c = (c_1,\\cdots,c_n)$ are the mixture components of the observations.  The mean-field variational family is described as follows:\n",
    "\n",
    "$$\n",
    "q(\\mu,c) = \\prod_{k=1}^K \\varphi_{m_k,s_k}(\\mu_k)\\prod_{i=1}^n \\mathrm{Cat}_{\\phi_i}(c_i)\\,, \n",
    "$$\n",
    "\n",
    "which means that:\n",
    "\n",
    "- $\\mu$ and $c$ are independent.\n",
    "- $(\\mu_{k})_{1\\leqslant k \\leqslant K}$ are independent with Gaussian distribution with means $(m_{k})_{1\\leqslant k \\leqslant K}$ and variances $(s_{k})_{1\\leqslant k \\leqslant K}$.\n",
    "- $(c_{i})_{1\\leqslant i \\leqslant n}$ are independent with multinomial distribution with parameters $(\\phi_i)_{1\\leqslant i \\leqslant n}$: $q(c_i=k) = \\phi_i(k)$ for $1\\leqslant k \\leqslant K$. \n",
    "\n",
    "Write $\\mathcal{D}$ the family of such distributions when $(m_{k})_{1\\leqslant k \\leqslant K}\\in \\mathbb{R}^K$, variances $(s_{k})_{1\\leqslant k \\leqslant K}\\in (\\mathbb{R}_+^*)^K$ and $(\\phi_i)_{1\\leqslant i \\leqslant n}\\in \\mathcal{S}_K^n$ where $\\mathcal{S}_K$ is the $K$-dimensional probability simplex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Initialize the values of $(m_{k})_{1\\leqslant k \\leqslant K}$, $(s_{k})_{1\\leqslant k \\leqslant K}$ and $(\\phi_i)_{1\\leqslant i \\leqslant n}$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "N = X.shape[0]\n",
    "\n",
    "phi = np.random.dirichlet([np.random.random()*np.random.randint(1, 10)]*K, N)\n",
    "m = np.random.normal(0, 1, K)\n",
    "s2 = np.ones(K) * np.random.random(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is now to find the\n",
    "\"best candidate\" in $\\mathcal{D}$ to approximate $p(\\mu,c|x)$, i.e. the one ``which minimizes the following KL divergence``:\n",
    "\n",
    "$$\n",
    "q^* = \\mathrm{Argmin}_{q\\in\\mathcal{D}} \\mathrm{KL}\\left(q(\\mu,c)\\|p(\\mu,c|x)\\right)\\,.\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\\begin{align*}\n",
    "\\mathrm{KL}\\left(q(\\mu,c)\\|p(\\mu,c|x)\\right) &= \\mathbb{E}_q[\\log q(\\mu,c)] - \\mathbb{E}_q[\\log p(\\mu,c|x)]\\,,\\\\\n",
    " &= \\mathbb{E}_q[\\log q(\\mu,c)] - \\mathbb{E}_q[\\log p(\\mu,c,x)]+\\log p(x)\\,,\\\\\n",
    "&= -\\mathrm{ELBO}(q)+\\log p(x)\\,,\n",
    "\\end{align*}\n",
    "\n",
    "where the ``Evidence Lower Bound`` (ELBO) is\n",
    "\n",
    "$$\n",
    "\\mathrm{ELBO}(q) = -\\mathbb{E}_q[\\log q(\\mu,c)] + \\mathbb{E}_q[\\log (\\mu,c,x)] \\,.\n",
    "$$\n",
    "\n",
    "Therefore, ``minimizing the KL divergence`` boils down to maximizing the ELBO, where $\\log p(x)\\geqslant \\mathrm{ELBO}(q)$.\n",
    "\n",
    "The complexity of the family $\\mathcal{D}$ determines the complexity of the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Generic CAVI algorithm.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detail here why the algorithm computes iteratively for all $1\\leqslant k \\leqslant K$,\n",
    "\n",
    "$$\n",
    "q(\\mu_k) \\propto \\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_k(\\mu_k|x)]\\right)\n",
    "$$\n",
    "\n",
    "and for all $1\\leqslant i \\leqslant n$,\n",
    "\n",
    "$$\n",
    "q(c_i) \\propto \\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)]\\right)\\,,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\tilde p_i(c_i|x)$ is the conditional distribution of $c_i$ given the observations and all the other parameters and $\\tilde p_k(\\mu_k|x)$ is the conditional distribution of $\\mu_k$ given the observations and all the other parameters.\n",
    "\n",
    "- $\\mathbb{E}_{\\tilde q_z}$ is the expectation under the variational law of all parameters except $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Application to the mixture of Gaussian distributions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\tilde p_i(c_i|x)$ be the conditional distribution of $c_i$ given the observations and the other parameters.\n",
    "\n",
    "$$\n",
    "\\tilde p_i(c_i|x) \\propto p(c_i)p(x_i|c_i,\\mu) \\propto p(c_i)\\prod_{k=1}^K \\left(\\varphi_{\\mu_k,1}(x_i)\\right)^{1_{c_i=k}}\\,. \n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)] = \\log p(c_i) + \\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[\\log \\varphi_{\\mu_k,1}]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)]\\right) &\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[-(x_i-\\mu_k)^2/2]\\right)\\,\\\\\n",
    "&\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[-(x_i-\\mu_k)^2/2]\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The update is then written:\n",
    "\n",
    "$$\n",
    "\\varphi_i(k) \\propto p(c_i=k) \\mathrm{exp}\\left(m_k x_i - \\frac{m_k + s_k}{2}\\right)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Update of $(\\phi_i)_{1\\leqslant i \\leqslant n}$ using CAVI.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_update_phi(X,m,s2):\n",
    "    \n",
    "    first_term_mean  = np.outer(X, m)\n",
    "    second_term_mean= -(m**2 + s2)/2\n",
    "    \n",
    "    phi = np.exp(first_term_mean + second_term_mean[np.newaxis, :])\n",
    "    phi = phi / phi.sum(1)[:, np.newaxis]\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\tilde p_k(\\mu_k|x)$ be the conditional distribution of $\\mu_k$ given the observations and the other parameters.\n",
    "\n",
    "$$\n",
    "\\tilde p_k(\\mu_k|x) \\propto p(\\mu_k)\\prod_{i=1}^np(x_i|c_i,\\mu) \\propto p(\\mu_k)\\prod_{i=1}^n p(x_i|\\mu,c_i)\\,. \n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_k(\\mu_k|x)] = \\log p(\\mu_k) + \\sum_{i=1}^n \\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log p(x_i|\\mu,c_i)]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_i(c_i|x)]\\right) &\\propto p(\\mu_k) \\mathrm{exp}\\left(\\sum_{i=1}^n\\sum_{k=1}^K  \\mathbb{E}_{\\tilde q_{\\mu_k}}[1_{c_i=k}\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto p(\\mu_k) \\mathrm{exp}\\left(\\sum_{i=1}^n \\phi_i(k) \\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto \\mathrm{exp}\\left(-\\frac{\\mu_k^2}{2\\sigma^2}-\\frac{1}{2}\\sum_{i=1}^n \\phi_i(k)(x_i-\\mu_k)^2\\right)\\,,\\\\\n",
    "&\\propto \\mathrm{exp}\\left(-\\frac{\\mu_k^2}{2\\sigma^2}+\\sum_{i=1}^n \\phi_i(k)x_i\\mu_k - \\frac{1}{2}\\sum_{i=1}^n \\phi_i(k)\\mu^2_k\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The update is then written:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{i=1}^n \\phi_i(k)x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_i(k)}\\quad\\mathrm{and}\\quad s_k = \\frac{1}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_i(k)}\\,. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Update of $(m_{k})_{1\\leqslant k \\leqslant K}$ and $(s_{k})_{1\\leqslant k \\leqslant K}$ using CAVI.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_update_mu(X,m,phi,s2,sigma2,K):\n",
    "    \n",
    "    m  = (phi*X[:, np.newaxis]).sum(0) * (1/sigma2 + phi.sum(0))**(-1)\n",
    "    s2 = (1/sigma2 + phi.sum(0))**(-1)\n",
    "    \n",
    "    return m, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(X,phi,m,s2,sigma2):\n",
    "        \n",
    "        first_term  = (np.log(s2) - m/sigma2).sum()\n",
    "        second_term = (-0.5*np.outer(X**2, s2+m**2) + np.outer(X, m) - np.log(phi))*phi\n",
    "\n",
    "        return first_term + second_term.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_mixture_Gaussian(X,m, s2, phi, sigma2, max_iter = 500, epsilon = 1e-8):\n",
    "        \n",
    "        elbos  = [elbo(X,phi,m,s2,sigma2)]\n",
    "        m_est  = [m]\n",
    "        s2_est = [s2]\n",
    "        \n",
    "        for it in range(1, max_iter+1):\n",
    "            \n",
    "            phi   = CAVI_update_phi(X,m,s2)\n",
    "            m, s2 = CAVI_update_mu(X,m,phi,s2,sigma2,K)\n",
    "            \n",
    "            m_est.append(m)\n",
    "            s2_est.append(s2)\n",
    "            \n",
    "            elbos.append(elbo(X,phi,m,s2,sigma2))\n",
    "\n",
    "            if np.abs(elbos[-2] - elbos[-1]) <= epsilon:\n",
    "                break\n",
    "        \n",
    "        return elbos, m_est, s2_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo(X, phi, m, s2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elbos, m_est, s2_est = CAVI_mixture_Gaussian(X, m, s2, phi, 1, 500, 0.0001)\n",
    "plt.plot(np.array(elbos),label = 'ELBO')\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
